{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dementia Template - Model.fit()\n",
    "\n",
    "This notebook features a Model class with a custom fit() function instead of the traditional gradient.tape() training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import losses, optimizers, metrics\n",
    "from tensorflow.keras import Input, Model, layers, callbacks, regularizers\n",
    "from jarvis.train import custom, params\n",
    "from jarvis.train.client import Client\n",
    "from jarvis.utils.general import gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define path of clients\n",
    "AD_CLIENT_PATH = '/data/raw/adni/data/ymls/client-3d-96x128_AD_only.yml'\n",
    "CN_CLIENT_PATH = '/data/raw/adni/data/ymls/client-3d-96x128_CN_only.yml'\n",
    "INPUT_SHAPE = (96, 128, 128, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Generators\n",
    "\n",
    "These custom generators yield batch sizes of 3 examples. It also ensures that the first example of the batch (index = 0) is AD and the last example of the batch (index = 2) is CN. The middle example (index = 1) can be either AD or CN at a 50% ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_generator(valid=False):\n",
    "    # --- Create generators for AD/CN \n",
    "    client_AD = Client(AD_CLIENT_PATH, configs = {'batch': {'size': p['batch_size'], 'fold': p['fold']}})\n",
    "    client_CN = Client(CN_CLIENT_PATH, configs = {'batch': {'size': p['batch_size'], 'fold': p['fold']}})\n",
    "    \n",
    "    gen_train_AD, gen_valid_AD = client_AD.create_generators()\n",
    "    gen_train_CN, gen_valid_CN = client_CN.create_generators()\n",
    "    \n",
    "    while True:\n",
    "        if valid:\n",
    "            xs_AD, ys_AD = next(gen_valid_AD)\n",
    "            xs_CN, ys_CN = next(gen_valid_CN)\n",
    "        else:\n",
    "            xs_AD, ys_AD = next(gen_train_AD)\n",
    "            xs_CN, ys_CN = next(gen_train_CN)\n",
    "        \n",
    "        # --- Randomize for AD-AD-CN or AD-CN-CN\n",
    "        choice_index = random.randint(0, 1)\n",
    "        \n",
    "        if choice_index == 0:\n",
    "            xs_final = np.concatenate((xs_AD['dat'][:2], xs_CN['dat'][:1]), axis=0)\n",
    "            ys_final = np.concatenate((ys_AD['lbl'][:2], ys_CN['lbl'][:1]), axis=0)\n",
    "        else:\n",
    "            xs_final = np.concatenate((xs_AD['dat'][:1], xs_CN['dat'][:2]), axis=0)\n",
    "            ys_final = np.concatenate((ys_AD['lbl'][:1], ys_CN['lbl'][:2]), axis=0)\n",
    "\n",
    "        xs = {}\n",
    "        ys = {}\n",
    "        \n",
    "        xs['pos'] = np.expand_dims(xs_final[0], axis=0)\n",
    "        xs['unk'] = np.expand_dims(xs_final[1], axis=0)\n",
    "        xs['neg'] = np.expand_dims(xs_final[2], axis=0)\n",
    "        ys['cls1'] = ys_final[0].reshape((1))\n",
    "        ys['cls2'] = ys_final[1].reshape((1))\n",
    "        ys['cls3'] = ys_final[2].reshape((1))\n",
    "        ys['dec1'] = np.expand_dims(xs_final[0], axis=0)\n",
    "        ys['dec2'] = np.expand_dims(xs_final[1], axis=0)\n",
    "        ys['dec3'] = np.expand_dims(xs_final[2], axis=0)\n",
    "        ys['ctr1'] = tf.dtypes.cast(ys['cls1'] == ys['cls2'], dtype=tf.float32)\n",
    "        ys['ctr2'] = tf.dtypes.cast(ys['cls2'] == ys['cls3'], dtype=tf.float32)\n",
    "            \n",
    "        yield xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Functions\n",
    "\n",
    "Custom loss functions defined below include cosine similarity, euclidean distance, and contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vects):\n",
    "    \"\"\"Find the cosine similarity between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing cosine similarity\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    a, b = vects\n",
    "    return 1 - tf.keras.layers.Dot(axes=1, normalize=True)([a, b])\n",
    "\n",
    "def euclidean_distance2(vects):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    a, b = vects\n",
    "    return tf.norm(a - b, ord='euclidean')\n",
    "\n",
    "def norm_euclidean_distance(vects):\n",
    "    \"\"\"Find the normalized Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing normalized euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    a, b = vects\n",
    "    return tf.norm(tf.nn.l2_normalize(a, 0) - tf.nn.l2_normalize(b, 0), ord='euclidean')\n",
    "\n",
    "def contrastive_loss(margin=1):\n",
    "    \"\"\"Provides 'ctr_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "    Arguments:\n",
    "        margin: Integer, defines the baseline for distance for which pairs\n",
    "                should be classified as dissimilar (default is 1). The\n",
    "                margin should correspond to the range of the distance function\n",
    "                used to compare the latent vectors.\n",
    "\n",
    "    Returns:\n",
    "        'ctr_loss' function with data ('margin') attached.\n",
    "\n",
    "    Resource:\n",
    "        https://www.pyimagesearch.com/2021/01/18/contrastive-loss-for-siamese-networks-with-keras-and-tensorflow/\n",
    "    \"\"\"\n",
    "    \n",
    "    def ctr_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the constrastive loss.\n",
    "\n",
    "        Arguments:\n",
    "            y_true: List of labels, each label is of type float32.\n",
    "            y_pred: List of predictions of same length as of y_true,\n",
    "                    each label is of type float32.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing constrastive loss as floating point value.\n",
    "        \"\"\"\n",
    "\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum((margin - y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            y_true * square_pred + (1 - y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return ctr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "The model receives as input a shape of `(batch_size, 96, 128, 128, 1)`. Assuming the batch_size is 3, each individual example from the batch is fed through an autoencoder tower, resulting in three autoencoder outputs. The three towers correspond to contrastive learning inputs (one positive class, one negative class, and one unknown class). The outputs of these three autoencoder towers are then passed to a contrastive learning model that performs euclidean distance / cosine similarity against the latent vectors of the positive-unknown and negative-unknown encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_block(input_layer, filters, kernel_size, strides):\n",
    "    conv1 = layers.Conv3D(filters=filters, kernel_size=kernel_size, strides=strides, activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    conv2 = layers.Conv3D(filters=filters, kernel_size=kernel_size, strides=strides, activation=\"relu\", padding=\"same\")(conv1)\n",
    "    return layers.Conv3D(filters=filters, kernel_size=kernel_size, strides=strides, activation=\"relu\", padding=\"same\")(conv2)\n",
    "\n",
    "def create_autoencoder(input_layer, num_blocks, filter_scale, kernel_size, strides, max_pool):\n",
    "    intermediate_layer = input_layer\n",
    "    downsample = []\n",
    "    for i in range(1, num_blocks+1):\n",
    "        intermediate_layer = unet_block(intermediate_layer, filter_scale*(i), kernel_size, strides)\n",
    "        downsample += [intermediate_layer]\n",
    "        intermediate_layer = layers.MaxPool3D(pool_size=max_pool)(intermediate_layer)\n",
    "    \n",
    "    enc_layer = layers.Conv3D(filters=filter_scale*num_blocks*2, kernel_size=kernel_size, strides=strides, activation=\"relu\", padding=\"same\")(intermediate_layer)\n",
    "    \n",
    "    flatten_layer = layers.Flatten()(enc_layer)\n",
    "    \n",
    "    cls_layer = layers.Dense(1, activation=\"sigmoid\")(flatten_layer)\n",
    "    \n",
    "    ctr_layer = layers.Dense(p['ctr_channels'], activation=\"sigmoid\")(flatten_layer)\n",
    "    \n",
    "    intermediate_layer = enc_layer\n",
    "    \n",
    "    for i in range(1, num_blocks+1):\n",
    "        intermediate_layer = layers.Conv3DTranspose(filters=filter_scale*((num_blocks+1)-i), kernel_size=kernel_size, strides=strides*2, activation=\"relu\", padding=\"same\")(intermediate_layer)\n",
    "        intermediate_layer = layers.Concatenate()([intermediate_layer, downsample[num_blocks-i]])\n",
    "        intermediate_layer = unet_block(intermediate_layer, filter_scale*((num_blocks+1)-i), kernel_size, strides)\n",
    "    \n",
    "    dec_layer = layers.Conv3D(filters=1, kernel_size=(1, 1, 1), strides=strides, activation=\"sigmoid\", padding=\"same\")(intermediate_layer)\n",
    "    \n",
    "    return [ctr_layer, cls_layer, dec_layer]\n",
    "\n",
    "\n",
    "def prepare_model(inputs, use_cosine_similarity=True):\n",
    "    inp = Input(INPUT_SHAPE)\n",
    "    \n",
    "    outputs = create_autoencoder(inp, p['num_blocks'], p['filter_scale'], p['kernel_size'], p['strides'], p['max_pool'])\n",
    "    \n",
    "    autoencoder_logits = {}\n",
    "    autoencoder_logits[\"ctr\"] = outputs[0]\n",
    "    autoencoder_logits[\"cls\"] = outputs[1]\n",
    "    autoencoder_logits[\"dec\"] = outputs[2]\n",
    "\n",
    "    autoencoder_network = Model(inputs=inp, outputs=autoencoder_logits)\n",
    "\n",
    "    tower_1 = autoencoder_network(inputs=inputs[\"pos\"])\n",
    "    tower_2 = autoencoder_network(inputs=inputs[\"unk\"])\n",
    "    tower_3 = autoencoder_network(inputs=inputs[\"neg\"])\n",
    "    \n",
    "    if use_cosine_similarity:\n",
    "        merge_layer1 = layers.Lambda(cosine_similarity)([tower_1[\"ctr\"], tower_2[\"ctr\"]])\n",
    "        merge_layer2 = layers.Lambda(cosine_similarity)([tower_2[\"ctr\"], tower_3[\"ctr\"]])\n",
    "    else:\n",
    "        merge_layer1 = layers.Lambda(euclidean_distance)([tower_1[\"ctr\"], tower_2[\"ctr\"]])\n",
    "        merge_layer2 = layers.Lambda(euclidean_distance)([tower_2[\"ctr\"], tower_3[\"ctr\"]])\n",
    "    \n",
    "    siamese_logits = {}\n",
    "    siamese_logits[\"ctr1\"] = merge_layer1\n",
    "    siamese_logits[\"ctr2\"] = merge_layer2\n",
    "    siamese_logits[\"cls1\"] = layers.Layer(name=\"cls1\")(tower_1[\"cls\"])\n",
    "    siamese_logits[\"cls2\"] = layers.Layer(name=\"cls2\")(tower_2[\"cls\"])\n",
    "    siamese_logits[\"cls3\"] = layers.Layer(name=\"cls3\")(tower_3[\"cls\"])\n",
    "    siamese_logits[\"dec1\"] = layers.Layer(name=\"dec1\")(tower_1[\"dec\"])\n",
    "    siamese_logits[\"dec2\"] = layers.Layer(name=\"dec2\")(tower_2[\"dec\"])\n",
    "    siamese_logits[\"dec3\"] = layers.Layer(name=\"dec3\")(tower_3[\"dec\"])\n",
    "    \n",
    "    siamese = Model(inputs=inputs, outputs=siamese_logits)\n",
    "    \n",
    "    siamese.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=p['LR']),\n",
    "        loss={\n",
    "            'ctr1': contrastive_loss(),\n",
    "            'ctr2': contrastive_loss(),\n",
    "            'dec1': losses.MeanSquaredError(),\n",
    "            'dec2': losses.MeanSquaredError(),\n",
    "            'dec3': losses.MeanSquaredError(),\n",
    "            'cls1': losses.BinaryCrossentropy(),\n",
    "            'cls2': losses.BinaryCrossentropy(),\n",
    "            'cls3': losses.BinaryCrossentropy()\n",
    "        },\n",
    "        loss_weights={\n",
    "            'ctr1': p['ctr1'],\n",
    "            'ctr2': p['ctr2'],\n",
    "            'dec1': p['dec1'],\n",
    "            'dec2': p['dec2'],\n",
    "            'dec3': p['dec3'],\n",
    "            'cls1': p['cls1'],\n",
    "            'cls2': p['cls2'],\n",
    "            'cls3': p['cls3']\n",
    "        },\n",
    "        metrics={\n",
    "            'cls1': metrics.BinaryAccuracy(),\n",
    "            'cls2': metrics.BinaryAccuracy(),\n",
    "            'cls3': metrics.BinaryAccuracy()\n",
    "        },\n",
    "        experimental_run_tf_function=False\n",
    "    )\n",
    "    \n",
    "    return siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2021-08-17 18:04:15 ] WARNING 1 GPU device(s) requested but only 0 available \n"
     ]
    }
   ],
   "source": [
    "# --- Autoselect GPU\n",
    "gpus.autoselect()\n",
    "\n",
    "# --- Prepare hyperparams\n",
    "p = params.load('./hyper.csv', row=0)\n",
    "\n",
    "MODEL_NAME = '{}/model.hdf5'.format(p['output_dir'])\n",
    "\n",
    "# --- Prepare model\n",
    "inputs = {\n",
    "    'pos': Input(shape=INPUT_SHAPE, name='pos'),\n",
    "    'unk': Input(shape=INPUT_SHAPE, name='unk'),\n",
    "    'neg': Input(shape=INPUT_SHAPE, name='neg'),\n",
    "}\n",
    "\n",
    "gen_train = contrastive_generator()\n",
    "gen_valid = contrastive_generator(valid=True)\n",
    "\n",
    "model = prepare_model(inputs, use_cosine_similarity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/100 [..............................] - ETA: 44:11 - loss: 3.3265 - cls1_loss: 0.6921 - cls2_loss: 0.6939 - cls3_loss: 0.6939 - lambda_loss: 1.0000 - lambda_1_loss: 0.0000e+00 - dec1_loss: 1.1446 - dec2_loss: 1.3107 - dec3_loss: 1.1178 - cls1_binary_accuracy: 1.0000 - cls2_binary_accuracy: 0.0000e+00 - cls3_binary_accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ecbf4ee5c6ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# --- Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Set training variables\n",
    "steps_per_epoch = p['steps_per_epoch']\n",
    "validation_freq = 1\n",
    "\n",
    "# --- Determine total loop iterations needed\n",
    "epochs = int(p['iterations'] / steps_per_epoch)\n",
    "\n",
    "# --- Prepare Tensorboard \n",
    "log_dir = '{}/jmodels/logdirs/{}'.format(\n",
    "    os.path.dirname(p['output_dir']),\n",
    "    os.path.basename(p['output_dir']))\n",
    "\n",
    "\n",
    "# --- Define training callbacks\n",
    "tensorboard_log = callbacks.TensorBoard(log_dir=log_dir, profile_batch=0)\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_decay = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# --- Train the model\n",
    "model.fit(\n",
    "    x=gen_train,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=steps_per_epoch,\n",
    "    validation_freq=validation_freq,\n",
    "    callbacks=[tensorboard_log]\n",
    ")\n",
    "\n",
    "# --- Save model\n",
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
