{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dementia Template - Model.fit()\n",
    "\n",
    "This notebook features a Model class with a custom fit() function instead of the traditional gradient.tape() training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import losses, optimizers, metrics\n",
    "from tensorflow.keras import Input, Model, layers, callbacks, regularizers\n",
    "from jarvis.train import custom, datasets, params\n",
    "from jarvis.train.client import Client\n",
    "from jarvis.utils.general import gpus, overload, tools as jtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define path of clients\n",
    "AD_CLIENT_PATH = '/data/raw/adni/data/ymls/client-3d-96x128_AD_only.yml'\n",
    "CN_CLIENT_PATH = '/data/raw/adni/data/ymls/client-3d-96x128_CN_only.yml'\n",
    "INPUT_SHAPE = (96, 128, 128, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Generators\n",
    "\n",
    "These custom generators yield batch sizes of 3 examples. It also ensures that the first example of the batch (index = 0) is AD and the last example of the batch (index = 2) is CN. The middle example (index = 1) can be either AD or CN at a 50% ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_generator(valid=False):\n",
    "    # --- Create generators for AD/CN \n",
    "    client_AD = Client(AD_CLIENT_PATH, configs = {'batch': {'size': p['batch_size'], 'fold': p['fold']}})\n",
    "    client_CN = Client(CN_CLIENT_PATH, configs = {'batch': {'size': p['batch_size'], 'fold': p['fold']}})\n",
    "    \n",
    "    gen_train_AD, gen_valid_AD = client_AD.create_generators()\n",
    "    gen_train_CN, gen_valid_CN = client_CN.create_generators()\n",
    "    \n",
    "    while True:\n",
    "        if valid:\n",
    "            xs_AD, ys_AD = next(gen_valid_AD)\n",
    "            xs_CN, ys_CN = next(gen_valid_CN)\n",
    "        else:\n",
    "            xs_AD, ys_AD = next(gen_train_AD)\n",
    "            xs_CN, ys_CN = next(gen_train_CN)\n",
    "        \n",
    "        # --- Randomize for AD-AD-CN or AD-CN-CN\n",
    "        choice_index = random.randint(0, 1)\n",
    "        \n",
    "        if choice_index == 0:\n",
    "            xs_final = np.concatenate((xs_AD['dat'], xs_CN['dat'][:1]), axis=0)\n",
    "            ys_final = np.concatenate((ys_AD['lbl'], ys_CN['lbl'][:1]), axis=0)\n",
    "        else:\n",
    "            xs_final = np.concatenate((xs_AD['dat'][:1], xs_CN['dat']), axis=0)\n",
    "            ys_final = np.concatenate((ys_AD['lbl'][:1], ys_CN['lbl']), axis=0)\n",
    "\n",
    "        xs = {}\n",
    "        ys = {}\n",
    "        \n",
    "        xs['pos'] = np.expand_dims(xs_final[0], axis=0)\n",
    "        xs['unk'] = np.expand_dims(xs_final[1], axis=0)\n",
    "        xs['neg'] = np.expand_dims(xs_final[2], axis=0)\n",
    "        ys['enc1'] = ys_final[0].reshape((1))\n",
    "        ys['enc2'] = ys_final[1].reshape((1))\n",
    "        ys['enc3'] = ys_final[2].reshape((1))\n",
    "        ys['dec1'] = np.expand_dims(xs_final[0], axis=0)\n",
    "        ys['dec2'] = np.expand_dims(xs_final[1], axis=0)\n",
    "        ys['dec3'] = np.expand_dims(xs_final[2], axis=0)\n",
    "        ys['ctr1'] = ys['enc1'] == ys['enc2']\n",
    "        ys['ctr2'] = ys['enc2'] == ys['enc3']\n",
    "            \n",
    "        yield xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Functions\n",
    "\n",
    "Custom loss functions defined below include cosine similarity, euclidean distance, and contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vects):\n",
    "    \"\"\"Find the cosine similarity between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing cosine similarity\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    a, b = vects\n",
    "    return 1 - tf.keras.layers.Dot(axes=1, normalize=True)([a, b])\n",
    "\n",
    "def euclidean_distance2(vects):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    a, b = vects\n",
    "    return tf.norm(a - b, ord='euclidean')\n",
    "\n",
    "def norm_euclidean_distance(vects):\n",
    "    \"\"\"Find the normalized Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing normalized euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    a, b = vects\n",
    "    return tf.norm(tf.nn.l2_normalize(a, 0) - tf.nn.l2_normalize(b, 0), ord='euclidean')\n",
    "\n",
    "def contrastive_loss(margin=1):\n",
    "    \"\"\"Provides 'ctr_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "    Arguments:\n",
    "        margin: Integer, defines the baseline for distance for which pairs\n",
    "                should be classified as dissimilar (default is 1). The\n",
    "                margin should correspond to the range of the distance function\n",
    "                used to compare the latent vectors.\n",
    "\n",
    "    Returns:\n",
    "        'ctr_loss' function with data ('margin') attached.\n",
    "\n",
    "    Resource:\n",
    "        https://www.pyimagesearch.com/2021/01/18/contrastive-loss-for-siamese-networks-with-keras-and-tensorflow/\n",
    "    \"\"\"\n",
    "    \n",
    "    def ctr_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the constrastive loss.\n",
    "\n",
    "        Arguments:\n",
    "            y_true: List of labels, each label is of type float32.\n",
    "            y_pred: List of predictions of same length as of y_true,\n",
    "                    each label is of type float32.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing constrastive loss as floating point value.\n",
    "        \"\"\"\n",
    "\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum((margin - y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            y_true * square_pred + (1 - y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return ctr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "The model receives as input a shape of `(batch_size, 96, 128, 128, 1)`. Assuming the batch_size is 3, each individual example from the batch is fed through an autoencoder tower, resulting in three autoencoder outputs. The three towers correspond to contrastive learning inputs (one positive class, one negative class, and one unknown class). The outputs of these three autoencoder towers are then passed to a contrastive learning model that performs euclidean distance / cosine similarity against the latent vectors of the positive-unknown and negative-unknown encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(inputs, use_cosine_similarity=True):\n",
    "        \n",
    "    # --- Define lambda functions\n",
    "    \n",
    "    kwargs = {\n",
    "        'kernel_size': (3, 3, 3),\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': 'he_uniform'\n",
    "    }\n",
    "\n",
    "    conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "    norm = lambda x : layers.BatchNormalization()(x)\n",
    "    acti = lambda x : layers.LeakyReLU()(x)\n",
    "    tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
    "    \n",
    "    conv1 = lambda filters, x : norm(acti(conv(x, filters, strides=1)))\n",
    "    conv2 = lambda filters, x : norm(acti(conv(x, filters, strides=(2, 2, 2))))\n",
    "    tran2 = lambda filters, x : norm(acti(tran(x, filters, strides=(2, 2, 2))))\n",
    "    \n",
    "    # --- Define autoencoder network\n",
    "    \n",
    "    inp = Input(INPUT_SHAPE)\n",
    "    e1 = conv1(4, inp)\n",
    "    e2 = conv1(8, conv2(8, e1))\n",
    "    e3 = conv1(16, conv2(16, e2))\n",
    "    e4 = conv1(32, conv2(32, e3))\n",
    "    e5 = layers.Conv3D(filters=4, kernel_size=(1, 1, 1))(e4)\n",
    "    e6 = layers.Flatten()(e5)\n",
    "    e7 = layers.Dense(10, activation=\"relu\", name=\"ctr\")(e6)\n",
    "    e8 = layers.Dense(1, activation=\"sigmoid\", name=\"enc\")(e7)\n",
    "    d1 = tran2(16, e4)\n",
    "    d2 = conv1(8, tran2(8, d1))\n",
    "    d3 = conv1(4, tran2(8, d2))\n",
    "    d4 = layers.Conv3D(filters=1, kernel_size=(1, 1, 1), name=\"dec\")(d3)\n",
    "    \n",
    "    autoencoder_logits = {}\n",
    "    autoencoder_logits[\"ctr\"] = e7\n",
    "    autoencoder_logits[\"enc\"] = e8\n",
    "    autoencoder_logits[\"dec\"] = d4\n",
    "    \n",
    "    autoencoder_network = Model(inputs=inp, outputs=autoencoder_logits)\n",
    "    \n",
    "    # --- Define contrastive network\n",
    "\n",
    "    tower_1 = autoencoder_network(inputs=inputs[\"pos\"])\n",
    "    tower_2 = autoencoder_network(inputs=inputs[\"unk\"])\n",
    "    tower_3 = autoencoder_network(inputs=inputs[\"neg\"])\n",
    "    \n",
    "    if use_cosine_similarity:\n",
    "        merge_layer1 = layers.Lambda(cosine_similarity)([tower_1[\"ctr\"], tower_2[\"ctr\"]])\n",
    "        merge_layer2 = layers.Lambda(cosine_similarity)([tower_2[\"ctr\"], tower_3[\"ctr\"]])\n",
    "    else:\n",
    "        merge_layer1 = layers.Lambda(euclidean_distance)([tower_1[\"ctr\"], tower_2[\"ctr\"]])\n",
    "        merge_layer2 = layers.Lambda(euclidean_distance)([tower_2[\"ctr\"], tower_3[\"ctr\"]])\n",
    "    \n",
    "    siamese_logits = {}\n",
    "    siamese_logits[\"ctr1\"] = layers.Dense(1, activation=\"sigmoid\", name=\"ctr1\")(merge_layer1)\n",
    "    siamese_logits[\"ctr2\"] = layers.Dense(1, activation=\"sigmoid\", name=\"ctr2\")(merge_layer2)\n",
    "    siamese_logits[\"enc1\"] = layers.Layer(name=\"enc1\")(tower_1[\"enc\"])\n",
    "    siamese_logits[\"enc2\"] = layers.Layer(name=\"enc2\")(tower_2[\"enc\"])\n",
    "    siamese_logits[\"enc3\"] = layers.Layer(name=\"enc3\")(tower_3[\"enc\"])\n",
    "    siamese_logits[\"dec1\"] = layers.Layer(name=\"dec1\")(tower_1[\"dec\"])\n",
    "    siamese_logits[\"dec2\"] = layers.Layer(name=\"dec2\")(tower_2[\"dec\"])\n",
    "    siamese_logits[\"dec3\"] = layers.Layer(name=\"dec3\")(tower_3[\"dec\"])\n",
    "    \n",
    "    siamese = Model(inputs=inputs, outputs=siamese_logits)\n",
    "    \n",
    "    siamese.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'ctr1': contrastive_loss(),\n",
    "            'ctr2': contrastive_loss(),\n",
    "            'dec1': losses.MeanSquaredError(),\n",
    "            'dec2': losses.MeanSquaredError(),\n",
    "            'dec3': losses.MeanSquaredError(),\n",
    "            'enc1': losses.BinaryCrossentropy(),\n",
    "            'enc2': losses.BinaryCrossentropy(),\n",
    "            'enc3': losses.BinaryCrossentropy()\n",
    "        },\n",
    "        loss_weights={\n",
    "            'ctr1': 0.5,\n",
    "            'ctr2': 0.5,\n",
    "            'dec1': 0.5,\n",
    "            'dec2': 0.5,\n",
    "            'dec3': 0.5,\n",
    "            'enc1': 0.5,\n",
    "            'enc2': 0.5,\n",
    "            'enc3': 0.5\n",
    "        },\n",
    "        metrics={\n",
    "            'enc1': metrics.BinaryAccuracy(),\n",
    "            'enc2': metrics.BinaryAccuracy(),\n",
    "            'enc3': metrics.BinaryAccuracy()\n",
    "        },\n",
    "        experimental_run_tf_function=False\n",
    "    )\n",
    "    \n",
    "    return siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2021-08-01 12:52:17 ] WARNING 1 GPU device(s) requested but only 0 available \n"
     ]
    }
   ],
   "source": [
    "# --- Autoselect GPU\n",
    "gpus.autoselect()\n",
    "\n",
    "# --- Prepare hyperparams\n",
    "p = params.load('./hyper.csv', row=0)\n",
    "\n",
    "MODEL_NAME = '{}/model.hdf5'.format(p['output_dir'])\n",
    "\n",
    "# --- Prepare model\n",
    "inputs = {\n",
    "    'pos': Input(shape=INPUT_SHAPE, name='pos'),\n",
    "    'unk': Input(shape=INPUT_SHAPE, name='unk'),\n",
    "    'neg': Input(shape=INPUT_SHAPE, name='neg'),\n",
    "}\n",
    "\n",
    "gen_train = contrastive_generator()\n",
    "gen_valid = contrastive_generator(valid=True)\n",
    "\n",
    "model = prepare_model(inputs, use_cosine_similarity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 8s 8s/step - loss: 1.9445 - ctr1_loss: 0.2534 - ctr2_loss: 0.2485 - dec1_loss: 1.0780 - dec2_loss: 0.7194 - dec3_loss: 0.7138 - enc1_loss: 0.4538 - enc2_loss: 0.4861 - enc3_loss: 0.5314 - enc1_binary_accuracy: 1.0000 - enc2_binary_accuracy: 1.0000 - enc3_binary_accuracy: 1.0000\n",
      "1/1 [==============================] - 31s 31s/step - loss: 4.5300 - ctr1_loss: 0.2008 - ctr2_loss: 0.1751 - dec1_loss: 2.0574 - dec2_loss: 2.2103 - dec3_loss: 1.8307 - enc1_loss: 2.7144 - enc2_loss: 0.5163 - enc3_loss: 0.2833 - enc1_binary_accuracy: 0.0000e+00 - enc2_binary_accuracy: 1.0000 - enc3_binary_accuracy: 1.0000 - val_loss: 1.9445 - val_ctr1_loss: 0.2534 - val_ctr2_loss: 0.2485 - val_dec1_loss: 1.0780 - val_dec2_loss: 0.7194 - val_dec3_loss: 0.7138 - val_enc1_loss: 0.4538 - val_enc2_loss: 0.4861 - val_enc3_loss: 0.5314 - val_enc1_binary_accuracy: 1.0000 - val_enc2_binary_accuracy: 1.0000 - val_enc3_binary_accuracy: 1.0000\n",
      "Epoch 2/200\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6293 - ctr1_loss: 0.2179 - ctr2_loss: 0.2447 - dec1_loss: 1.2264 - dec2_loss: 0.8333 - dec3_loss: 1.3168 - enc1_loss: 0.5999 - enc2_loss: 0.7607 - enc3_loss: 0.7606 - enc1_binary_accuracy: 1.0000 - enc2_binary_accuracy: 0.0000e+00 - enc3_binary_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 7s 7s/step - loss: 4.7633 - ctr1_loss: 0.2493 - ctr2_loss: 0.2481 - dec1_loss: 1.8599 - dec2_loss: 1.8872 - dec3_loss: 1.9154 - enc1_loss: 0.0870 - enc2_loss: 2.2465 - enc3_loss: 2.2480 - enc1_binary_accuracy: 1.0000 - enc2_binary_accuracy: 0.0000e+00 - enc3_binary_accuracy: 0.0000e+00 - val_loss: 2.6293 - val_ctr1_loss: 0.2179 - val_ctr2_loss: 0.2447 - val_dec1_loss: 1.2264 - val_dec2_loss: 0.8333 - val_dec3_loss: 1.3168 - val_enc1_loss: 0.5999 - val_enc2_loss: 0.7607 - val_enc3_loss: 0.7606 - val_enc1_binary_accuracy: 1.0000 - val_enc2_binary_accuracy: 0.0000e+00 - val_enc3_binary_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5567 - ctr1_loss: 0.2473 - ctr2_loss: 0.2302 - dec1_loss: 1.0062 - dec2_loss: 1.3826 - dec3_loss: 0.8697 - enc1_loss: 0.6512 - enc2_loss: 0.7366 - enc3_loss: 0.6921 - enc1_binary_accuracy: 1.0000 - enc2_binary_accuracy: 0.0000e+00 - enc3_binary_accuracy: 1.0000\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.8065 - ctr1_loss: 0.2507 - ctr2_loss: 0.2494 - dec1_loss: 1.7895 - dec2_loss: 1.5337 - dec3_loss: 1.7132 - enc1_loss: 0.2324 - enc2_loss: 0.1405 - enc3_loss: 0.0979 - enc1_binary_accuracy: 1.0000 - enc2_binary_accuracy: 1.0000 - enc3_binary_accuracy: 1.0000 - val_loss: 2.5567 - val_ctr1_loss: 0.2473 - val_ctr2_loss: 0.2302 - val_dec1_loss: 1.0062 - val_dec2_loss: 1.3826 - val_dec3_loss: 0.8697 - val_enc1_loss: 0.6512 - val_enc2_loss: 0.7366 - val_enc3_loss: 0.6921 - val_enc1_binary_accuracy: 1.0000 - val_enc2_binary_accuracy: 0.0000e+00 - val_enc3_binary_accuracy: 1.0000\n",
      "Epoch 4/200\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6830 - ctr1_loss: 0.2461 - ctr2_loss: 0.2181 - dec1_loss: 1.0087 - dec2_loss: 0.7865 - dec3_loss: 1.5099 - enc1_loss: 0.6836 - enc2_loss: 0.7754 - enc3_loss: 0.8849 - enc1_binary_accuracy: 1.0000 - enc2_binary_accuracy: 0.0000e+00 - enc3_binary_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.9951 - ctr1_loss: 0.1851 - ctr2_loss: 0.2349 - dec1_loss: 1.3758 - dec2_loss: 1.5192 - dec3_loss: 1.4502 - enc1_loss: 0.4617 - enc2_loss: 0.6930 - enc3_loss: 0.6914 - enc1_binary_accuracy: 1.0000 - enc2_binary_accuracy: 1.0000 - enc3_binary_accuracy: 1.0000 - val_loss: 2.6830 - val_ctr1_loss: 0.2461 - val_ctr2_loss: 0.2181 - val_dec1_loss: 1.0087 - val_dec2_loss: 0.7865 - val_dec3_loss: 1.5099 - val_enc1_loss: 0.6836 - val_enc2_loss: 0.7754 - val_enc3_loss: 0.8849 - val_enc1_binary_accuracy: 1.0000 - val_enc2_binary_accuracy: 0.0000e+00 - val_enc3_binary_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3316 - ctr1_loss: 0.2419 - ctr2_loss: 0.2383 - dec1_loss: 0.8720 - dec2_loss: 0.7131 - dec3_loss: 1.0397 - enc1_loss: 0.6317 - enc2_loss: 0.9461 - enc3_loss: 0.7301 - enc1_binary_accuracy: 1.0000 - enc2_binary_accuracy: 0.0000e+00 - enc3_binary_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.3066 - ctr1_loss: 0.2648 - ctr2_loss: 0.2488 - dec1_loss: 1.3326 - dec2_loss: 1.2979 - dec3_loss: 1.1946 - enc1_loss: 0.3458 - enc2_loss: 0.1832 - enc3_loss: 0.1993 - enc1_binary_accuracy: 1.0000 - enc2_binary_accuracy: 1.0000 - enc3_binary_accuracy: 1.0000 - val_loss: 2.3316 - val_ctr1_loss: 0.2419 - val_ctr2_loss: 0.2383 - val_dec1_loss: 0.8720 - val_dec2_loss: 0.7131 - val_dec3_loss: 1.0397 - val_enc1_loss: 0.6317 - val_enc2_loss: 0.9461 - val_enc3_loss: 0.7301 - val_enc1_binary_accuracy: 1.0000 - val_enc2_binary_accuracy: 0.0000e+00 - val_enc3_binary_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3125 - ctr1_loss: 0.2073 - ctr2_loss: 0.2475 - dec1_loss: 0.8461 - dec2_loss: 1.0365 - dec3_loss: 1.0321 - enc1_loss: 0.3829 - enc2_loss: 0.7484 - enc3_loss: 0.7793 - enc1_binary_accuracy: 1.0000 - enc2_binary_accuracy: 0.0000e+00 - enc3_binary_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 7s 7s/step - loss: 2.0691 - ctr1_loss: 0.2534 - ctr2_loss: 0.2365 - dec1_loss: 1.2436 - dec2_loss: 1.1311 - dec3_loss: 1.2902 - enc1_loss: 0.0609 - enc2_loss: 0.0885 - enc3_loss: 0.1972 - enc1_binary_accuracy: 1.0000 - enc2_binary_accuracy: 1.0000 - enc3_binary_accuracy: 1.0000 - val_loss: 2.3125 - val_ctr1_loss: 0.2073 - val_ctr2_loss: 0.2475 - val_dec1_loss: 0.8461 - val_dec2_loss: 1.0365 - val_dec3_loss: 1.0321 - val_enc1_loss: 0.3829 - val_enc2_loss: 0.7484 - val_enc3_loss: 0.7793 - val_enc1_binary_accuracy: 1.0000 - val_enc2_binary_accuracy: 0.0000e+00 - val_enc3_binary_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-47503af28169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_log\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1121\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Set training variables\n",
    "steps_per_epoch = 100\n",
    "validation_freq = 1\n",
    "\n",
    "# --- Determine total loop iterations needed\n",
    "epochs = int(p['iterations'] / steps_per_epoch)\n",
    "\n",
    "# --- Prepare Tensorboard \n",
    "log_dir = '{}/jmodels/logdirs/{}'.format(\n",
    "    os.path.dirname(p['output_dir']),\n",
    "    os.path.basename(p['output_dir']))\n",
    "\n",
    "# --- Prepare CSV path\n",
    "csv_log_path = '{}/train_log.csv'.format(p['output_dir'])\n",
    "\n",
    "# --- Define training callbacks\n",
    "tensorboard_log = callbacks.TensorBoard(log_dir=log_dir, profile_batch=0)\n",
    "csv_log = callbacks.CSVLogger(csv_log_path)\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_decay = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# --- Train the model\n",
    "model.fit(\n",
    "    x=gen_train,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=steps_per_epoch,\n",
    "    validation_freq=validation_freq,\n",
    "    callbacks=[tensorboard_log, csv_log]\n",
    ")\n",
    "\n",
    "# --- Save model\n",
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
