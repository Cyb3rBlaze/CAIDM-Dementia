{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dementia Template - Dummy Data + 2 Towers\n",
    "\n",
    "Dementia template with dummy data using two autoencoder towers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "from tensorflow import losses, optimizers, metrics\n",
    "from tensorflow.keras import Input, Model, layers, callbacks, regularizers\n",
    "from jarvis.train import custom, datasets, params\n",
    "from jarvis.train.client import Client\n",
    "from jarvis.utils.general import gpus, overload, tools as jtools\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(Model):\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            sample_weight = None\n",
    "            x, y = data\n",
    "        \n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "#             print(\"Shapes:\")\n",
    "            tf.print(\"\\n shape:\", x.shape, y.shape, output_stream=sys.stdout)\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value.\n",
    "            # The loss function is configured in `compile()`.\n",
    "            loss = self.compiled_loss(\n",
    "                y,\n",
    "                y_pred,\n",
    "                sample_weight=sample_weight,\n",
    "                regularization_losses=self.losses,\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics.\n",
    "        # Metrics are configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    lo = -0.5\n",
    "    hi = +0.5\n",
    "    rand = lambda shape : np.random.rand(*shape) * (hi - lo) + lo\n",
    "\n",
    "    pos = rand((96, 128, 128, 1))\n",
    "    neg = rand((96, 128, 128, 1))\n",
    "    unk = rand((96, 128, 128, 1))\n",
    "    inputs = {\"pos\": Input(pos.shape, name='pos'), \"neg\": Input(neg.shape, name='neg'), \"unk\": Input(unk.shape, name='unk')}\n",
    "    \n",
    "    dat = rand((96, 128, 128, 1))\n",
    "    inputs = {\"dat\": Input(dat.shape, batch_size=3, name='dat')}\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vects):\n",
    "    \"\"\"Find the cosine similarity between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing cosine similarity\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y = vects\n",
    "    \n",
    "    x = tf.math.l2_normalize(x, axis=1)\n",
    "    y = tf.math.l2_normalize(y, axis=1)\n",
    "    return -tf.math.reduce_mean(x * y, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(margin=1):\n",
    "    \"\"\"Provides 'constrastive_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "  Arguments:\n",
    "      margin: Integer, defines the baseline for distance for which pairs\n",
    "              should be classified as dissimilar. - (default is 1).\n",
    "\n",
    "  Returns:\n",
    "      'constrastive_loss' function with data ('margin') attached.\n",
    "  \"\"\"\n",
    "\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the constrastive loss.\n",
    "\n",
    "      Arguments:\n",
    "          y_true: List of labels, each label is of type float32.\n",
    "          y_pred: List of predictions of same length as of y_true,\n",
    "                  each label is of type float32.\n",
    "\n",
    "      Returns:\n",
    "          A tensor containing constrastive loss as floating point value.\n",
    "      \"\"\"\n",
    "\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return contrastive_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(inputs):\n",
    "    \n",
    "    # --- Define lambda functions\n",
    "    \n",
    "    kwargs = {\n",
    "        'kernel_size': (3, 3, 3),\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': 'he_uniform'\n",
    "    }\n",
    "    conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "    norm = lambda x : layers.BatchNormalization()(x)\n",
    "    acti = lambda x : layers.LeakyReLU()(x)\n",
    "    tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
    "    \n",
    "    conv1 = lambda filters, x : norm(acti(conv(x, filters, strides=1)))\n",
    "    conv2 = lambda filters, x : norm(acti(conv(x, filters, strides=(2, 2, 2))))\n",
    "    tran2 = lambda filters, x : norm(acti(tran(x, filters, strides=(2, 2, 2))))\n",
    "    \n",
    "    # --- Define autoencoder network\n",
    "    \n",
    "    inp = Input((96, 128, 128, 1))\n",
    "    e1 = conv1(4, inp)\n",
    "    e2 = conv1(8, conv2(8, e1))\n",
    "    e3 = conv1(16, conv2(16, e2))\n",
    "    e4 = conv1(32, conv2(32, e3))\n",
    "    e5 = layers.Conv3D(filters=4, kernel_size=(1, 1, 1))(e4)\n",
    "    e6 = layers.Flatten()(e5)\n",
    "    e7 = layers.Dense(10, activation='sigmoid', name=\"enc\")(e6)\n",
    "    d1 = tran2(16, e4)\n",
    "    d2 = conv1(8, tran2(8, d1))\n",
    "    d3 = conv1(4, tran2(8, d2))\n",
    "    d4 = layers.Conv3D(filters=1, kernel_size=(1, 1, 1), name=\"dec\")(d3)\n",
    "    \n",
    "    autoencoder_logits = {}\n",
    "    autoencoder_logits[\"enc\"] = e7\n",
    "    autoencoder_logits[\"dec\"] = d4\n",
    "    \n",
    "    autoencoder_network = CustomModel(inputs=inp, outputs=autoencoder_logits)\n",
    "    \n",
    "    # --- Define contrastive network\n",
    "    \n",
    "    inp_1 = Input((96, 128, 128, 1), name=\"inp_1\")\n",
    "    inp_2 = Input((96, 128, 128, 1), name=\"inp_2\")\n",
    "    \n",
    "    tower_1 = autoencoder_network(inp_1)\n",
    "    tower_2 = autoencoder_network(inp_2)\n",
    "    \n",
    "    merge_layer = layers.Lambda(cosine_similarity)([tower_1[\"enc\"], tower_2[\"enc\"]])\n",
    "#     merge_layer = layers.Lambda(euclidean_distance)([tower_1[\"enc\"], tower_2[\"enc\"]])\n",
    "    normal_layer = layers.BatchNormalization()(merge_layer)\n",
    "    \n",
    "    siamese_logits = {}\n",
    "    siamese_logits[\"ctr\"] = layers.Dense(1, activation=\"sigmoid\", name=\"ctr\")(normal_layer)\n",
    "    siamese_logits[\"enc\"] = layers.Layer(name=\"enc\")(tower_1[\"enc\"])\n",
    "    siamese_logits[\"dec\"] = layers.Layer(name=\"dec\")(tower_2[\"dec\"])\n",
    "    \n",
    "    siamese = CustomModel(inputs=[inp_1, inp_2], outputs=siamese_logits)\n",
    "    \n",
    "    siamese.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'ctr': loss(),\n",
    "            'dec': losses.MeanSquaredError(),\n",
    "            'enc': losses.BinaryCrossentropy()\n",
    "        },\n",
    "        metrics={\n",
    "            'enc': metrics.Accuracy(),\n",
    "        },\n",
    "        experimental_run_tf_function=False\n",
    "    )\n",
    "    \n",
    "    return siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model(get_inputs())\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(batch_size=128):\n",
    "    \"\"\"\n",
    "    Method to define a Python generator for training data\n",
    "    \n",
    "    \"\"\"\n",
    "    # --- Define lambda function for random values [-0.5, +0.5]\n",
    "    lo = -0.5\n",
    "    hi = +0.5\n",
    "    rand = lambda shape : np.random.rand(*shape) * (hi - lo) + lo\n",
    "\n",
    "    # --- Define lambda function for linear transform\n",
    "    m = 2\n",
    "    b = -1\n",
    "    f = lambda x : m * x + b + rand((1, 96, 128, 128, 1))\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        xs = {}\n",
    "        xs['inp_1'] = np.expand_dims(rand((3, 96, 128, 128)), -1)\n",
    "        xs['inp_2'] = np.expand_dims(rand((3, 96, 128, 128)), -1)\n",
    "        \n",
    "#         xs['dat'] = ((batch_size, 3, 96, 160, 160, 1))\n",
    "\n",
    "        ys = {}\n",
    "        ys['ctr'] = rand((3, 1))\n",
    "        ys['enc'] = rand((3, 10))\n",
    "        ys['dec'] = rand((3, 96, 128, 128, 1))\n",
    "                \n",
    "        yield xs, ys\n",
    "        \n",
    "gen_train = Generator()\n",
    "gen_valid = Generator()\n",
    "gen_test = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 59s 12s/step - loss: 2.5024 - ctr_loss: 0.2481 - dec_loss: 1.4003 - enc_loss: 0.8541 - enc_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efed82df6a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_logger = callbacks.CSVLogger(filename=\"training_log.csv\")\n",
    "\n",
    "# --- Train\n",
    "model.fit(\n",
    "    x=gen_train,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=5,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=10,\n",
    "    validation_freq=5#,\n",
    "#     callbacks=[csv_logger]\n",
    "#     callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn={\n",
    "    'ctr': loss(),\n",
    "    'dec': losses.MeanSquaredError(),\n",
    "    'enc': losses.BinaryCrossentropy()\n",
    "}\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = metrics.Accuracy()\n",
    "val_acc_metric = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "tf.Tensor(0.3676393, shape=(), dtype=float32)\n",
      "Training loss (for one batch) at step 0: 0.3676\n",
      "Seen so far: 3 samples\n",
      "tf.Tensor(0.21232478, shape=(), dtype=float32)\n",
      "Training loss (for one batch) at step 1: 0.2123\n",
      "Seen so far: 6 samples\n",
      "tf.Tensor(0.36463028, shape=(), dtype=float32)\n",
      "Training loss (for one batch) at step 2: 0.3646\n",
      "Seen so far: 9 samples\n",
      "tf.Tensor(0.21238667, shape=(), dtype=float32)\n",
      "Training loss (for one batch) at step 3: 0.2124\n",
      "Seen so far: 12 samples\n",
      "tf.Tensor(0.38951963, shape=(), dtype=float32)\n",
      "Training loss (for one batch) at step 4: 0.3895\n",
      "Seen so far: 15 samples\n",
      "tf.Tensor(0.487056, shape=(), dtype=float32)\n",
      "Training loss (for one batch) at step 5: 0.4871\n",
      "Seen so far: 18 samples\n",
      "tf.Tensor(0.07119781, shape=(), dtype=float32)\n",
      "Training loss (for one batch) at step 6: 0.0712\n",
      "Seen so far: 21 samples\n",
      "tf.Tensor(0.3659007, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-776c4c0781fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0menc_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdec_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mctr_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv3DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    157\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    160\u001b[0m       nn_ops.conv3d_backprop_filter_v2(\n\u001b[1;32m    161\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv3d_backprop_input_v2\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dilations\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1871\u001b[0;31m         dilations)\n\u001b[0m\u001b[1;32m   1872\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(gen_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            enc_loss = loss_fn['enc'](y_batch_train['enc'], logits['enc'])\n",
    "            dec_loss = loss_fn['dec'](y_batch_train['dec'], logits['dec'])\n",
    "            ctr_loss = loss_fn['ctr'](y_batch_train['ctr'], logits['ctr'])\n",
    "            loss_value = 0.5 * enc_loss + 0.2 * dec_loss + 0.3 * ctr_loss\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train['enc'], logits['enc'])\n",
    "\n",
    "        # Log every 200 batches.\n",
    "#         if step % 200 == 0:\n",
    "        print(\n",
    "            \"Training loss (for one batch) at step %d: %.4f\"\n",
    "            % (step, float(loss_value))\n",
    "        )\n",
    "        print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "#     for x_batch_val, y_batch_val in val_dataset:\n",
    "#         val_logits = model(x_batch_val, training=False)\n",
    "#         # Update val metrics\n",
    "#         val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "#     val_acc = val_acc_metric.result()\n",
    "#     val_acc_metric.reset_states()\n",
    "#     print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
