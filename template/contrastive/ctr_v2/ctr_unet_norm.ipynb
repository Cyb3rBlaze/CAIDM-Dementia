{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dementia Template - Model.fit()\n",
    "\n",
    "This notebook features a Model class with a custom fit() function instead of the traditional gradient.tape() training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import losses, optimizers, metrics\n",
    "from tensorflow.keras import Input, Model, layers, callbacks, regularizers\n",
    "from jarvis.train import custom, params\n",
    "from jarvis.train.client import Client\n",
    "from jarvis.utils.general import overload, gpus\n",
    "from jarvis.utils.display import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define path of clients\n",
    "AD_CLIENT_PATH = '/home/mmorelan/proj/dementia/yml/client-3d-96x128_AD_AV45_only.yml'\n",
    "CN_CLIENT_PATH = '/home/mmorelan/proj/dementia/yml/client-3d-96x128_CN_AV45_only.yml'\n",
    "INPUT_SHAPE = (96, 128, 128, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Generators\n",
    "\n",
    "These custom generators yield batch sizes of 3 examples. It also ensures that the first example of the batch (index = 0) is AD and the last example of the batch (index = 2) is CN. The middle example (index = 1) can be either AD or CN at a 50% ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@overload(Client)\n",
    "def preprocess(self, arrays, **kwargs):\n",
    "    \n",
    "    # --- Extract pre-calculated whole exam mu/sd and normalize\n",
    "    arrays['xs']['dat'] = (arrays['xs']['dat'] - kwargs['row']['mu']) / kwargs['row']['sd']\n",
    "    \n",
    "    # --- Scale to 0/1 using 5/95 percentiles\n",
    "    lower = np.percentile(arrays['xs']['dat'], 1)\n",
    "    upper = np.percentile(arrays['xs']['dat'], 99)\n",
    "    arrays['xs']['dat'] = arrays['xs']['dat'].clip(min=lower, max=upper)\n",
    "    arrays['xs']['dat'] = (arrays['xs']['dat'] - lower) / (upper - lower)\n",
    "    \n",
    "    return arrays\n",
    "\n",
    "def contrastive_generator(valid=False):\n",
    "    # --- Create generators for AD/CN \n",
    "    client_AD = Client(AD_CLIENT_PATH, configs = {'batch': {'size': p['batch_size'], 'fold': p['fold']}})\n",
    "    client_CN = Client(CN_CLIENT_PATH, configs = {'batch': {'size': p['batch_size'], 'fold': p['fold']}})\n",
    "    \n",
    "    gen_train_AD, gen_valid_AD = client_AD.create_generators()\n",
    "    gen_train_CN, gen_valid_CN = client_CN.create_generators()\n",
    "    \n",
    "    while True:\n",
    "        if valid:\n",
    "            xs_AD, ys_AD = next(gen_valid_AD)\n",
    "            xs_CN, ys_CN = next(gen_valid_CN)\n",
    "        else:\n",
    "            xs_AD, ys_AD = next(gen_train_AD)\n",
    "            xs_CN, ys_CN = next(gen_train_CN)\n",
    "        \n",
    "        # --- Randomize for AD-AD-CN or AD-CN-CN\n",
    "        choice_index = random.randint(0, 1)\n",
    "        \n",
    "        if choice_index == 0:\n",
    "            xs_final = np.concatenate((xs_AD['dat'][:2], xs_CN['dat'][:1]), axis=0)\n",
    "            ys_final = np.concatenate((ys_AD['lbl'][:2], ys_CN['lbl'][:1]), axis=0)\n",
    "        else:\n",
    "            xs_final = np.concatenate((xs_AD['dat'][:1], xs_CN['dat'][:2]), axis=0)\n",
    "            ys_final = np.concatenate((ys_AD['lbl'][:1], ys_CN['lbl'][:2]), axis=0)\n",
    "\n",
    "        xs = {}\n",
    "        ys = {}\n",
    "        \n",
    "        xs['pos'] = np.expand_dims(xs_final[0], axis=0)\n",
    "        xs['unk'] = np.expand_dims(xs_final[1], axis=0)\n",
    "        xs['neg'] = np.expand_dims(xs_final[2], axis=0)\n",
    "        ys['cls1'] = ys_final[0].reshape((1))\n",
    "        ys['cls2'] = ys_final[1].reshape((1))\n",
    "        ys['cls3'] = ys_final[2].reshape((1))\n",
    "        ys['dec1'] = np.expand_dims(xs_final[0], axis=0)\n",
    "        ys['dec2'] = np.expand_dims(xs_final[1], axis=0)\n",
    "        ys['dec3'] = np.expand_dims(xs_final[2], axis=0)\n",
    "        ys['ctr1'] = tf.dtypes.cast(ys['cls1'] == ys['cls2'], dtype=tf.float32)\n",
    "        ys['ctr2'] = tf.dtypes.cast(ys['cls2'] == ys['cls3'], dtype=tf.float32)\n",
    "            \n",
    "        yield xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Functions\n",
    "\n",
    "Custom loss functions defined below include cosine similarity, euclidean distance, and contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vects):\n",
    "    \"\"\"Find the cosine similarity between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing cosine similarity\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    a, b = vects\n",
    "    return 1 - tf.keras.layers.Dot(axes=1, normalize=True)([a, b])\n",
    "\n",
    "def euclidean_distance2(vects):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    a, b = vects\n",
    "    return tf.norm(a - b, ord='euclidean')\n",
    "\n",
    "def norm_euclidean_distance(vects):\n",
    "    \"\"\"Find the normalized Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing normalized euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    a, b = vects\n",
    "    return tf.norm(tf.nn.l2_normalize(a, 0) - tf.nn.l2_normalize(b, 0), ord='euclidean')\n",
    "\n",
    "def contrastive_loss(margin=1):\n",
    "    \"\"\"Provides 'ctr_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "    Arguments:\n",
    "        margin: Integer, defines the baseline for distance for which pairs\n",
    "                should be classified as dissimilar (default is 1). The\n",
    "                margin should correspond to the range of the distance function\n",
    "                used to compare the latent vectors.\n",
    "\n",
    "    Returns:\n",
    "        'ctr_loss' function with data ('margin') attached.\n",
    "\n",
    "    Resource:\n",
    "        https://www.pyimagesearch.com/2021/01/18/contrastive-loss-for-siamese-networks-with-keras-and-tensorflow/\n",
    "    \"\"\"\n",
    "    \n",
    "    def ctr_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the constrastive loss.\n",
    "\n",
    "        Arguments:\n",
    "            y_true: List of labels, each label is of type float32.\n",
    "            y_pred: List of predictions of same length as of y_true,\n",
    "                    each label is of type float32.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing constrastive loss as floating point value.\n",
    "        \"\"\"\n",
    "\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum((margin - y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            y_true * square_pred + (1 - y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return ctr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "The model receives as input a shape of `(batch_size, 96, 128, 128, 1)`. Assuming the batch_size is 3, each individual example from the batch is fed through an autoencoder tower, resulting in three autoencoder outputs. The three towers correspond to contrastive learning inputs (one positive class, one negative class, and one unknown class). The outputs of these three autoencoder towers are then passed to a contrastive learning model that performs euclidean distance / cosine similarity against the latent vectors of the positive-unknown and negative-unknown encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_block(input_layer, filters, kernel_size, strides):\n",
    "    conv1 = layers.Conv3D(filters=filters, kernel_size=kernel_size, strides=strides, activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    conv2 = layers.Conv3D(filters=filters, kernel_size=kernel_size, strides=strides, activation=\"relu\", padding=\"same\")(conv1)\n",
    "    return layers.Conv3D(filters=filters, kernel_size=kernel_size, strides=strides, activation=\"relu\", padding=\"same\")(conv2)\n",
    "\n",
    "def create_autoencoder(input_layer, num_blocks, filter_scale, kernel_size, strides, max_pool):\n",
    "    intermediate_layer = input_layer\n",
    "    downsample = []\n",
    "    for i in range(1, num_blocks+1):\n",
    "        intermediate_layer = unet_block(intermediate_layer, filter_scale*(i), kernel_size, strides)\n",
    "        downsample += [intermediate_layer]\n",
    "        intermediate_layer = layers.MaxPool3D(pool_size=max_pool)(intermediate_layer)\n",
    "    \n",
    "    enc_layer = layers.Conv3D(filters=filter_scale*num_blocks*2, kernel_size=kernel_size, strides=strides, activation=\"relu\", padding=\"same\")(intermediate_layer)\n",
    "    \n",
    "    flatten_layer = layers.Flatten()(enc_layer)\n",
    "    \n",
    "    cls_layer = layers.Dense(1, activation=\"sigmoid\")(flatten_layer)\n",
    "    \n",
    "    ctr_layer = layers.Dense(p['ctr_channels'], activation=\"sigmoid\")(flatten_layer)\n",
    "    \n",
    "    intermediate_layer = enc_layer\n",
    "    \n",
    "    for i in range(1, num_blocks+1):\n",
    "        intermediate_layer = layers.Conv3DTranspose(filters=filter_scale*((num_blocks+1)-i), kernel_size=kernel_size, strides=strides*2, activation=\"relu\", padding=\"same\")(intermediate_layer)\n",
    "        intermediate_layer = layers.Concatenate()([intermediate_layer, downsample[num_blocks-i]])\n",
    "        intermediate_layer = unet_block(intermediate_layer, filter_scale*((num_blocks+1)-i), kernel_size, strides)\n",
    "    \n",
    "    dec_layer = layers.Conv3D(filters=1, kernel_size=(1, 1, 1), strides=strides, activation=\"sigmoid\", padding=\"same\")(intermediate_layer)\n",
    "    \n",
    "    return [ctr_layer, cls_layer, dec_layer]\n",
    "\n",
    "\n",
    "def prepare_model(inputs, use_cosine_similarity=True):\n",
    "    inp = Input(INPUT_SHAPE)\n",
    "    \n",
    "    outputs = create_autoencoder(inp, p['num_blocks'], p['filter_scale'], p['kernel_size'], p['strides'], p['max_pool'])\n",
    "    \n",
    "    autoencoder_logits = {}\n",
    "    autoencoder_logits[\"ctr\"] = outputs[0]\n",
    "    autoencoder_logits[\"cls\"] = outputs[1]\n",
    "    autoencoder_logits[\"dec\"] = outputs[2]\n",
    "\n",
    "    autoencoder_network = Model(inputs=inp, outputs=autoencoder_logits)\n",
    "\n",
    "    tower_1 = autoencoder_network(inputs=inputs[\"pos\"])\n",
    "    tower_2 = autoencoder_network(inputs=inputs[\"unk\"])\n",
    "    tower_3 = autoencoder_network(inputs=inputs[\"neg\"])\n",
    "    \n",
    "    if use_cosine_similarity:\n",
    "        merge_layer1 = layers.Lambda(cosine_similarity)([tower_1[\"ctr\"], tower_2[\"ctr\"]])\n",
    "        merge_layer2 = layers.Lambda(cosine_similarity)([tower_2[\"ctr\"], tower_3[\"ctr\"]])\n",
    "    else:\n",
    "        merge_layer1 = layers.Lambda(euclidean_distance)([tower_1[\"ctr\"], tower_2[\"ctr\"]])\n",
    "        merge_layer2 = layers.Lambda(euclidean_distance)([tower_2[\"ctr\"], tower_3[\"ctr\"]])\n",
    "    \n",
    "    siamese_logits = {}\n",
    "    siamese_logits[\"ctr1\"] = layers.Layer(name=\"ctr1\")(merge_layer1)\n",
    "    siamese_logits[\"ctr2\"] = layers.Layer(name=\"ctr2\")(merge_layer2)\n",
    "    siamese_logits[\"cls1\"] = layers.Layer(name=\"cls1\")(tower_1[\"cls\"])\n",
    "    siamese_logits[\"cls2\"] = layers.Layer(name=\"cls2\")(tower_2[\"cls\"])\n",
    "    siamese_logits[\"cls3\"] = layers.Layer(name=\"cls3\")(tower_3[\"cls\"])\n",
    "    siamese_logits[\"dec1\"] = layers.Layer(name=\"dec1\")(tower_1[\"dec\"])\n",
    "    siamese_logits[\"dec2\"] = layers.Layer(name=\"dec2\")(tower_2[\"dec\"])\n",
    "    siamese_logits[\"dec3\"] = layers.Layer(name=\"dec3\")(tower_3[\"dec\"])\n",
    "    \n",
    "    siamese = Model(inputs=inputs, outputs=siamese_logits)\n",
    "    \n",
    "    siamese.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=p['LR']),\n",
    "        loss={\n",
    "            'ctr1': contrastive_loss(),\n",
    "            'ctr2': contrastive_loss(),\n",
    "            'dec1': losses.MeanSquaredError(),\n",
    "            'dec2': losses.MeanSquaredError(),\n",
    "            'dec3': losses.MeanSquaredError(),\n",
    "            'cls1': losses.BinaryCrossentropy(),\n",
    "            'cls2': losses.BinaryCrossentropy(),\n",
    "            'cls3': losses.BinaryCrossentropy()\n",
    "        },\n",
    "        loss_weights={\n",
    "            'ctr1': p['ctr1'],\n",
    "            'ctr2': p['ctr2'],\n",
    "            'dec1': p['dec1'],\n",
    "            'dec2': p['dec2'],\n",
    "            'dec3': p['dec3'],\n",
    "            'cls1': p['cls1'],\n",
    "            'cls2': p['cls2'],\n",
    "            'cls3': p['cls3']\n",
    "        },\n",
    "        metrics={\n",
    "            'cls1': metrics.BinaryAccuracy(),\n",
    "            'cls2': metrics.BinaryAccuracy(),\n",
    "            'cls3': metrics.BinaryAccuracy()\n",
    "        },\n",
    "        experimental_run_tf_function=False\n",
    "    )\n",
    "    \n",
    "    return siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2021-08-20 16:31:33 ] CUDA_VISIBLE_DEVICES already manually set to: -1       \n"
     ]
    }
   ],
   "source": [
    "# --- Autoselect GPU\n",
    "gpus.autoselect()\n",
    "\n",
    "# --- Prepare hyperparams\n",
    "p = params.load('./hyper.csv', row=0)\n",
    "\n",
    "MODEL_NAME = '{}/model.hdf5'.format(p['output_dir'])\n",
    "\n",
    "# --- Prepare model\n",
    "inputs = {\n",
    "    'pos': Input(shape=INPUT_SHAPE, name='pos'),\n",
    "    'unk': Input(shape=INPUT_SHAPE, name='unk'),\n",
    "    'neg': Input(shape=INPUT_SHAPE, name='neg'),\n",
    "}\n",
    "\n",
    "gen_train = contrastive_generator()\n",
    "gen_valid = contrastive_generator(valid=True)\n",
    "\n",
    "model = prepare_model(inputs, use_cosine_similarity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Extract autoencoder model from contrastive model\n",
    "cls_model = model.layers[3]\n",
    "\n",
    "for x, y in gen_train:\n",
    "    x = x['pos']\n",
    "    logits = cls_model.predict(x)\n",
    "    dec_logits = logits['dec']\n",
    "    print(dec_logits.shape)\n",
    "    imshow(dec_logits, figsize=(12, 12))\n",
    "    plt.show()\n",
    "    print(np.mean(dec_logits))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 [==============================] - 31s 17s/step - loss: 1.7198 - cls1_loss: 0.7116 - cls2_loss: 0.6750 - cls3_loss: 0.6750 - lambda_2_loss: 1.0000 - lambda_3_loss: 8.8818e-15 - dec1_loss: 0.1254 - dec2_loss: 0.1242 - dec3_loss: 0.1283 - cls1_binary_accuracy: 0.0000e+00 - cls2_binary_accuracy: 1.0000 - cls3_binary_accuracy: 1.0000 - val_loss: 1.6959 - val_cls1_loss: 0.7161 - val_cls2_loss: 0.6934 - val_cls3_loss: 0.6707 - val_lambda_2_loss: 0.5000 - val_lambda_3_loss: 0.5000 - val_dec1_loss: 0.1053 - val_dec2_loss: 0.0983 - val_dec3_loss: 0.1080 - val_cls1_binary_accuracy: 0.0000e+00 - val_cls2_binary_accuracy: 0.5000 - val_cls3_binary_accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomCallback' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-c5a6e2a75f8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# --- Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1227\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/jarvis/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-f19131a365c5>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCustomCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomCallback' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# --- Set training variables\n",
    "steps_per_epoch = 2\n",
    "validation_freq = 1\n",
    "\n",
    "# --- Determine total loop iterations needed\n",
    "# epochs = int(p['iterations'] / steps_per_epoch)\n",
    "epochs = 5\n",
    "\n",
    "# --- Prepare Tensorboard \n",
    "log_dir = '{}/jmodels/logdirs/{}'.format(\n",
    "    os.path.dirname(p['output_dir']),\n",
    "    os.path.basename(p['output_dir']))\n",
    "\n",
    "# --- Define training callbacks\n",
    "tensorboard_log = callbacks.TensorBoard(log_dir=log_dir, profile_batch=0)\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_decay = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# --- Train the model\n",
    "model.fit(\n",
    "    x=gen_train,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=steps_per_epoch,\n",
    "    validation_freq=validation_freq,\n",
    "    callbacks=[tensorboard_log]\n",
    ")\n",
    "\n",
    "# --- Save model\n",
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
