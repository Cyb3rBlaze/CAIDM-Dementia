{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dementia Template - gradient.tape()\n",
    "\n",
    "This notebook features a custom training script using gradient.tape() instead of Model.fit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import losses, optimizers, metrics\n",
    "from tensorflow.keras import Input, Model, layers, callbacks, regularizers\n",
    "from jarvis.train import custom, datasets, params\n",
    "from jarvis.train.client import Client\n",
    "from jarvis.utils.general import gpus, overload, tools as jtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define path of clients\n",
    "AD_CLIENT_PATH = '/data/raw/adni/data/ymls/client-3d-96x128_AD_only.yml'\n",
    "CN_CLIENT_PATH = '/data/raw/adni/data/ymls/client-3d-96x128_CN_only.yml'\n",
    "INPUT_SHAPE = (96, 128, 128, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Generators\n",
    "\n",
    "These custom generators yield batch sizes of 3 examples. It also ensures that the first example of the batch (index = 0) is AD and the last example of the batch (index = 2) is CN. The middle example (index = 1) can be either AD or CN at a 50% ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_generator():\n",
    "    # --- Create generators for AD/CN \n",
    "    client_AD = Client(AD_CLIENT_PATH)\n",
    "    client_CN = Client(CN_CLIENT_PATH)\n",
    "    \n",
    "    gen_train_AD, gen_valid_AD = client_AD.create_generators()\n",
    "    gen_train_CN, gen_valid_CN = client_CN.create_generators()\n",
    "    \n",
    "    while True:\n",
    "        xs_AD, ys_AD = next(gen_valid_AD)\n",
    "        xs_CN, ys_CN = next(gen_valid_CN)\n",
    "        \n",
    "        # --- Randomize for AD-AD-CN or AD-CN-CN\n",
    "        choice_index = random.randint(0, 1)\n",
    "        \n",
    "        if choice_index == 0:\n",
    "            xs_final = np.concatenate((xs_AD['dat'], xs_CN['dat'][:1]), axis=0)\n",
    "            ys_final = np.concatenate((ys_AD['lbl'], ys_CN['lbl'][:1]), axis=0)\n",
    "        else:\n",
    "            xs_final = np.concatenate((xs_AD['dat'][:1], xs_CN['dat']), axis=0)\n",
    "            ys_final = np.concatenate((ys_AD['lbl'][:1], ys_CN['lbl']), axis=0)\n",
    "            \n",
    "        yield xs_final, ys_final\n",
    "\n",
    "def train_generator():\n",
    "    # --- Create generators for AD/CN \n",
    "    client_AD = Client(AD_CLIENT_PATH)\n",
    "    client_CN = Client(CN_CLIENT_PATH)\n",
    "    \n",
    "    gen_train_AD, gen_valid_CN = client_AD.create_generators()\n",
    "    gen_train_CN, gen_valid_CN = client_CN.create_generators()\n",
    "    \n",
    "    while True:\n",
    "        xs_AD, ys_AD = next(gen_train_AD)\n",
    "        xs_CN, ys_CN = next(gen_train_CN)\n",
    "        \n",
    "        # --- Randomize for AD-AD-CN or AD-CN-CN\n",
    "        choice_index = random.randint(0, 1)\n",
    "        \n",
    "        if choice_index == 0:\n",
    "            xs_final = np.concatenate((xs_AD['dat'], xs_CN['dat'][:1]), axis=0)\n",
    "            ys_final = np.concatenate((ys_AD['lbl'], ys_CN['lbl'][:1]), axis=0)\n",
    "        else:\n",
    "            xs_final = np.concatenate((xs_AD['dat'][:1], xs_CN['dat']), axis=0)\n",
    "            ys_final = np.concatenate((ys_AD['lbl'][:1], ys_CN['lbl']), axis=0)\n",
    "            \n",
    "        yield xs_final, ys_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Functions\n",
    "\n",
    "Custom loss functions defined below include cosine similarity, euclidean distance, and contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vects):\n",
    "    \"\"\"Find the cosine similarity between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing cosine similarity\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y = vects\n",
    "    \n",
    "    x = tf.math.l2_normalize(x, axis=1)\n",
    "    y = tf.math.l2_normalize(y, axis=1)\n",
    "    return -tf.math.reduce_mean(x * y, axis=1, keepdims=True)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "def loss(margin=1):\n",
    "    \"\"\"Provides 'constrastive_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "  Arguments:\n",
    "      margin: Integer, defines the baseline for distance for which pairs\n",
    "              should be classified as dissimilar. - (default is 1).\n",
    "\n",
    "  Returns:\n",
    "      'constrastive_loss' function with data ('margin') attached.\n",
    "  \"\"\"\n",
    "\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the constrastive loss.\n",
    "\n",
    "      Arguments:\n",
    "          y_true: List of labels, each label is of type float32.\n",
    "          y_pred: List of predictions of same length as of y_true,\n",
    "                  each label is of type float32.\n",
    "\n",
    "      Returns:\n",
    "          A tensor containing constrastive loss as floating point value.\n",
    "      \"\"\"\n",
    "\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "The model receives as input a shape of `(batch_size, 96, 128, 128, 1)`. Assuming the batch_size is 3, each individual example from the batch is fed through an autoencoder tower, resulting in three autoencoder outputs. The three towers correspond to contrastive learning inputs (one positive class, one negative class, and one unknown class). The outputs of these three autoencoder towers are then passed to a contrastive learning model that performs euclidean distance / cosine similarity against the latent vectors of the positive-unknown and negative-unknown encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(inputs, use_cosine_similarity=True, use_normalization=True):\n",
    "        \n",
    "    # --- Define lambda functions\n",
    "    \n",
    "    kwargs = {\n",
    "        'kernel_size': (3, 3, 3),\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': 'he_uniform'\n",
    "    }\n",
    "    conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "    norm = lambda x : layers.BatchNormalization()(x)\n",
    "    acti = lambda x : layers.LeakyReLU()(x)\n",
    "    tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
    "    \n",
    "    conv1 = lambda filters, x : norm(acti(conv(x, filters, strides=1)))\n",
    "    conv2 = lambda filters, x : norm(acti(conv(x, filters, strides=(2, 2, 2))))\n",
    "    tran2 = lambda filters, x : norm(acti(tran(x, filters, strides=(2, 2, 2))))\n",
    "    \n",
    "    # --- Define autoencoder network\n",
    "    \n",
    "    inp = Input(INPUT_SHAPE)\n",
    "    e1 = conv1(4, inp)\n",
    "    e2 = conv1(8, conv2(8, e1))\n",
    "    e3 = conv1(16, conv2(16, e2))\n",
    "    e4 = conv1(32, conv2(32, e3))\n",
    "    e5 = layers.Conv3D(filters=4, kernel_size=(1, 1, 1))(e4)\n",
    "    e6 = layers.Flatten()(e5)\n",
    "    e7 = layers.Dense(10, activation='relu', name=\"ctr\")(e6)\n",
    "    e8 = layers.Dense(1, activation='sigmoid', name=\"enc\")(e7)\n",
    "    d1 = tran2(16, e4)\n",
    "    d2 = conv1(8, tran2(8, d1))\n",
    "    d3 = conv1(4, tran2(8, d2))\n",
    "    d4 = layers.Conv3D(filters=1, kernel_size=(1, 1, 1), name=\"dec\")(d3)\n",
    "    \n",
    "    autoencoder_logits = {}\n",
    "    autoencoder_logits[\"ctr\"] = e7\n",
    "    autoencoder_logits[\"enc\"] = e8\n",
    "    autoencoder_logits[\"dec\"] = d4\n",
    "    \n",
    "    autoencoder_network = Model(inputs=inp, outputs=autoencoder_logits)\n",
    "    \n",
    "    # --- Define contrastive network\n",
    "    \n",
    "    tower_1 = autoencoder_network(inputs[np.newaxis, 0, :])\n",
    "    tower_2 = autoencoder_network(inputs[np.newaxis, 1, :])\n",
    "    tower_3 = autoencoder_network(inputs[np.newaxis, 2, :])\n",
    "    \n",
    "    if use_cosine_similarity:\n",
    "        merge_layer1 = layers.Lambda(cosine_similarity)([tower_1[\"ctr\"], tower_2[\"ctr\"]])\n",
    "        merge_layer2 = layers.Lambda(cosine_similarity)([tower_2[\"ctr\"], tower_3[\"ctr\"]])\n",
    "    else:\n",
    "        merge_layer1 = layers.Lambda(euclidean_distance)([tower_1[\"ctr\"], tower_2[\"ctr\"]])\n",
    "        merge_layer2 = layers.Lambda(euclidean_distance)([tower_2[\"ctr\"], tower_3[\"ctr\"]])\n",
    "\n",
    "    if use_normalization:\n",
    "        normal_layer1 = layers.BatchNormalization()(merge_layer1)\n",
    "        normal_layer2 = layers.BatchNormalization()(merge_layer2)\n",
    "    else:\n",
    "        normal_layer1 = merge_layer1\n",
    "        normal_layer2 = merge_layer2\n",
    "    \n",
    "    siamese_logits = {}\n",
    "    siamese_logits[\"ctr1\"] = layers.Dense(1, activation=\"sigmoid\", name=\"ctr1\")(normal_layer1)\n",
    "    siamese_logits[\"ctr2\"] = layers.Dense(1, activation=\"sigmoid\", name=\"ctr2\")(normal_layer2)\n",
    "    siamese_logits[\"enc1\"] = layers.Layer(name=\"enc1\")(tower_1[\"enc\"])\n",
    "    siamese_logits[\"enc2\"] = layers.Layer(name=\"enc2\")(tower_2[\"enc\"])\n",
    "    siamese_logits[\"enc3\"] = layers.Layer(name=\"enc3\")(tower_3[\"enc\"])\n",
    "    siamese_logits[\"dec1\"] = layers.Layer(name=\"dec1\")(tower_1[\"dec\"])\n",
    "    siamese_logits[\"dec2\"] = layers.Layer(name=\"dec2\")(tower_2[\"dec\"])\n",
    "    siamese_logits[\"dec3\"] = layers.Layer(name=\"dec3\")(tower_3[\"dec\"])\n",
    "    \n",
    "    siamese = Model(inputs=inputs, outputs=siamese_logits)\n",
    "    \n",
    "    return siamese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Variables\n",
    "\n",
    "Load the hyperparameter configuration from a CSV file, define a model path to export the model binary during training, define the contrastive model inputs, and instantiate the contrastive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare hyperparams\n",
    "p = params.load('./hyper.csv', row=0)\n",
    "\n",
    "MODEL_NAME = '{}/model.hdf5'.format(p['output_dir'])\n",
    "\n",
    "# --- Prepare model\n",
    "inputs = Input(shape=INPUT_SHAPE, name='dat')\n",
    "model = prepare_model(inputs, use_cosine_similarity=True, use_normalization=False)\n",
    "\n",
    "# Instantiate an optimizer to train the model\n",
    "optimizer = optimizers.Adam(learning_rate=p['LR'])\n",
    "\n",
    "# Instantiate a loss function\n",
    "loss_fn = {\n",
    "    'ctr1': loss(),\n",
    "    'ctr2': loss(),\n",
    "    'dec1': losses.MeanSquaredError(),\n",
    "    'dec2': losses.MeanSquaredError(),\n",
    "    'dec3': losses.MeanSquaredError(),\n",
    "    'enc1': losses.BinaryCrossentropy(),\n",
    "    'enc2': losses.BinaryCrossentropy(),\n",
    "    'enc3': losses.BinaryCrossentropy()\n",
    "}\n",
    "\n",
    "# Prepare the metrics\n",
    "train_enc1_metric = metrics.BinaryAccuracy()\n",
    "train_enc2_metric = metrics.BinaryAccuracy()\n",
    "train_enc3_metric = metrics.BinaryAccuracy()\n",
    "val_enc1_metric = metrics.BinaryAccuracy()\n",
    "val_enc2_metric = metrics.BinaryAccuracy()\n",
    "val_enc3_metric = metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loop\n",
    "\n",
    "### Terminology\n",
    "\n",
    "* Batch size - the number of training examples in the batch that is passed to the model\n",
    "* Step - one gradient update where an amount of batch_size examples are processed\n",
    "* Epoch - one full cycle through the training data (epoch = # examples / batch_size)\n",
    "\n",
    "Source: https://tolotra.com/2018/07/25/what-is-the-difference-between-step-batch-size-epoch-iteration-machine-learning-terminology/\n",
    "\n",
    "### Description\n",
    "\n",
    "The custom training loop iterates over the data generators for a specified `STEPS_PER_EPOCH`. Toggling `debug=True` enables more verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs=1, steps_per_epoch=1, debug=False):\n",
    "    gen_train = train_generator()\n",
    "    gen_valid = valid_generator()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterate over the batches of the dataset\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(gen_train):\n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                if debug:\n",
    "                    print(\"y:\", np.squeeze(y_batch_train[0]), np.squeeze(y_batch_train[1]), np.squeeze(y_batch_train[2]))\n",
    "                    \n",
    "                logits = model(x_batch_train, training=True)\n",
    "\n",
    "                # Encoder loss\n",
    "                enc1_loss = loss_fn['enc1'](y_batch_train[0], logits['enc1'])\n",
    "                enc2_loss = loss_fn['enc2'](y_batch_train[1], logits['enc2'])\n",
    "                enc3_loss = loss_fn['enc3'](y_batch_train[2], logits['enc3'])\n",
    "\n",
    "                # Decoder loss\n",
    "                dec1_loss = loss_fn['dec1'](x_batch_train[0], logits['dec1'])\n",
    "                dec2_loss = loss_fn['dec2'](x_batch_train[1], logits['dec2'])\n",
    "                dec3_loss = loss_fn['dec3'](x_batch_train[2], logits['dec3'])\n",
    "\n",
    "                # Contrastive loss\n",
    "                ctr1_loss = loss_fn['ctr1'](y_batch_train[0] == y_batch_train[1], logits['ctr1'])\n",
    "                ctr2_loss = loss_fn['ctr2'](y_batch_train[1] == y_batch_train[2], logits['ctr2'])\n",
    "                \n",
    "                # Combine loss values\n",
    "                loss_value = enc1_loss + enc2_loss + enc3_loss + dec1_loss + dec2_loss + dec3_loss + ctr1_loss + ctr2_loss\n",
    "\n",
    "                # Print loss values of current batch\n",
    "                if debug:\n",
    "                    print(\"e1:\", K.get_value(enc1_loss), \"e2:\", K.get_value(enc2_loss), \"e3:\", K.get_value(enc3_loss))\n",
    "                    print(\"d1:\", K.get_value(dec1_loss), \"d2:\", K.get_value(dec2_loss), \"d3:\", K.get_value(dec3_loss))\n",
    "                    print(\"c1:\", K.get_value(ctr1_loss), \"c2:\", K.get_value(ctr2_loss))\n",
    "                    print(\"loss:\", K.get_value(loss_value))\n",
    "\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "                      \n",
    "            # Update training metrics\n",
    "            train_enc1_metric.update_state(y_batch_train[0], logits['enc1'])\n",
    "            train_enc2_metric.update_state(y_batch_train[1], logits['enc2'])\n",
    "            train_enc3_metric.update_state(y_batch_train[2], logits['enc3'])        \n",
    "            \n",
    "            if debug:\n",
    "                print(\"pred:\", K.get_value(logits['enc1']), \"true:\", np.squeeze(y_batch_train[0]))\n",
    "                print(\"pred:\", K.get_value(logits['enc2']), \"true:\", np.squeeze(y_batch_train[1]))\n",
    "                print(\"pred:\", K.get_value(logits['enc3']), \"true:\", np.squeeze(y_batch_train[2]))\n",
    "\n",
    "            # Log every steps (batch)\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * p['batch_size']))\n",
    "                     \n",
    "            # Quit if step reaches preset steps per epoch\n",
    "            if step == steps_per_epoch:\n",
    "                break\n",
    "\n",
    "        # Display metrics at the end of each epoch\n",
    "        train_enc1_acc = train_enc1_metric.result()\n",
    "        train_enc2_acc = train_enc2_metric.result()\n",
    "        train_enc3_acc = train_enc3_metric.result()\n",
    "        print(\"Training acc over epoch:\\n\", float(train_enc1_acc), \"\\n\", float(train_enc2_acc), \"\\n\", float(train_enc3_acc))\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_enc1_metric.reset_states()\n",
    "        train_enc2_metric.reset_states()\n",
    "        train_enc3_metric.reset_states()\n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        for step, (x_batch_val, y_batch_val) in enumerate(gen_valid):\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            \n",
    "            # Update val metrics\n",
    "            val_enc1_metric.update_state(y_batch_val[0], val_logits['enc1'])\n",
    "            val_enc2_metric.update_state(y_batch_val[1], val_logits['enc2'])\n",
    "            val_enc3_metric.update_state(y_batch_val[2], val_logits['enc3'])\n",
    "            \n",
    "            if step == steps_per_epoch:\n",
    "                break\n",
    "\n",
    "        val_enc1_acc = val_enc1_metric.result()\n",
    "        val_enc2_acc = val_enc2_metric.result()\n",
    "        val_enc3_acc = val_enc3_metric.result()\n",
    "        \n",
    "        val_enc1_metric.reset_states()\n",
    "        val_enc2_metric.reset_states()\n",
    "        val_enc3_metric.reset_states()\n",
    "        \n",
    "        print(\"Validation acc:\\n\", float(val_enc1_acc), \"\\n\", float(val_enc2_acc), \"\\n\", float(val_enc3_acc))\n",
    "        \n",
    "        # Output epoch time\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 16.7913\n",
      "Seen so far: 3 samples\n",
      "Training loss (for one batch) at step 1: 19.3677\n",
      "Seen so far: 6 samples\n",
      "Training loss (for one batch) at step 2: 15.3991\n",
      "Seen so far: 9 samples\n",
      "Training acc over epoch:\n",
      " 1.0 \n",
      " 0.6666666865348816 \n",
      " 0.3333333432674408\n",
      "Validation acc:\n",
      " 1.0 \n",
      " 0.3333333432674408 \n",
      " 0.0\n",
      "Time taken: 46.85s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 16.4920\n",
      "Seen so far: 3 samples\n",
      "Training loss (for one batch) at step 1: 16.2324\n",
      "Seen so far: 6 samples\n",
      "Training loss (for one batch) at step 2: 16.1885\n",
      "Seen so far: 9 samples\n",
      "Training acc over epoch:\n",
      " 1.0 \n",
      " 0.6666666865348816 \n",
      " 0.3333333432674408\n",
      "Validation acc:\n",
      " 1.0 \n",
      " 0.6666666865348816 \n",
      " 0.0\n",
      "Time taken: 46.73s\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs=2, steps_per_epoch=2, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
