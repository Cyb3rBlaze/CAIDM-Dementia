{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dementia Template - Dummy Data + 3 Towers\n",
    "\n",
    "Dementia template with dummy data using three autoencoder towers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "from tensorflow import losses, optimizers, metrics\n",
    "from tensorflow.keras import Input, Model, layers, callbacks, regularizers\n",
    "from jarvis.train import custom, datasets, params\n",
    "from jarvis.train.client import Client\n",
    "from jarvis.utils.general import gpus, overload, tools as jtools\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(Model):\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            sample_weight = None\n",
    "            x, y = data\n",
    "        \n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "#             print(\"Shapes:\")\n",
    "            tf.print(\"\\n shape:\", x.shape, y.shape, output_stream=sys.stdout)\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value.\n",
    "            # The loss function is configured in `compile()`.\n",
    "            loss = self.compiled_loss(\n",
    "                y,\n",
    "                y_pred,\n",
    "                sample_weight=sample_weight,\n",
    "                regularization_losses=self.losses,\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics.\n",
    "        # Metrics are configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    lo = -0.5\n",
    "    hi = +0.5\n",
    "    rand = lambda shape : np.random.rand(*shape) * (hi - lo) + lo\n",
    "\n",
    "    pos = rand((96, 160, 160, 1))\n",
    "    neg = rand((96, 160, 160, 1))\n",
    "    unk = rand((96, 160, 160, 1))\n",
    "    inputs = {\"pos\": Input(pos.shape, name='pos'), \"neg\": Input(neg.shape, name='neg'), \"unk\": Input(unk.shape, name='unk')}\n",
    "    \n",
    "    dat = rand((96, 160, 160, 1))\n",
    "    inputs = {\"dat\": Input(dat.shape, batch_size=3, name='dat')}\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vects):\n",
    "    \"\"\"Find the cosine similarity between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing cosine similarity\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y = vects\n",
    "    \n",
    "    x = tf.math.l2_normalize(x, axis=1)\n",
    "    y = tf.math.l2_normalize(y, axis=1)\n",
    "    return -tf.math.reduce_mean(x * y, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(margin=1):\n",
    "    \"\"\"Provides 'constrastive_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "  Arguments:\n",
    "      margin: Integer, defines the baseline for distance for which pairs\n",
    "              should be classified as dissimilar. - (default is 1).\n",
    "\n",
    "  Returns:\n",
    "      'constrastive_loss' function with data ('margin') attached.\n",
    "  \"\"\"\n",
    "\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the constrastive loss.\n",
    "\n",
    "      Arguments:\n",
    "          y_true: List of labels, each label is of type float32.\n",
    "          y_pred: List of predictions of same length as of y_true,\n",
    "                  each label is of type float32.\n",
    "\n",
    "      Returns:\n",
    "          A tensor containing constrastive loss as floating point value.\n",
    "      \"\"\"\n",
    "\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return contrastive_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(inputs):\n",
    "    \n",
    "    # --- Define lambda functions\n",
    "    \n",
    "    kwargs = {\n",
    "        'kernel_size': (3, 3, 3),\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': 'he_uniform'\n",
    "    }\n",
    "    conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "    norm = lambda x : layers.BatchNormalization()(x)\n",
    "    acti = lambda x : layers.LeakyReLU()(x)\n",
    "    tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
    "    \n",
    "    conv1 = lambda filters, x : norm(acti(conv(x, filters, strides=1)))\n",
    "    conv2 = lambda filters, x : norm(acti(conv(x, filters, strides=(2, 2, 2))))\n",
    "    tran2 = lambda filters, x : norm(acti(tran(x, filters, strides=(2, 2, 2))))\n",
    "    \n",
    "    # --- Define autoencoder network\n",
    "    \n",
    "    inp = Input((96, 160, 160, 1))\n",
    "    e1 = conv1(4, inp)\n",
    "    e2 = conv1(8, conv2(8, e1))\n",
    "    e3 = conv1(16, conv2(16, e2))\n",
    "    e4 = conv1(32, conv2(32, e3))\n",
    "    e5 = layers.Conv3D(filters=4, kernel_size=(1, 1, 1))(e4)\n",
    "    e6 = layers.Flatten()(e5)\n",
    "    e7 = layers.Dense(10, activation='sigmoid', name=\"enc\")(e6)\n",
    "    d1 = tran2(16, e4)\n",
    "    d2 = conv1(8, tran2(8, d1))\n",
    "    d3 = conv1(4, tran2(8, d2))\n",
    "    d4 = layers.Conv3D(filters=1, kernel_size=(1, 1, 1), name=\"dec\")(d3)\n",
    "    \n",
    "    autoencoder_logits = {}\n",
    "    autoencoder_logits[\"enc\"] = e7\n",
    "    autoencoder_logits[\"dec\"] = d4\n",
    "    \n",
    "    autoencoder_network = CustomModel(inputs=inp, outputs=autoencoder_logits)\n",
    "    \n",
    "    # --- Define contrastive network\n",
    "    \n",
    "    inp_1 = Input((96, 160, 160, 1), name=\"inp_1\")\n",
    "    inp_2 = Input((96, 160, 160, 1), name=\"inp_2\")\n",
    "    inp_3 = Input((96, 160, 160, 1), name=\"inp_3\")\n",
    "    \n",
    "    tower_1 = autoencoder_network(inp_1)\n",
    "    tower_2 = autoencoder_network(inp_2)\n",
    "    tower_3 = autoencoder_network(inp_3)\n",
    "    \n",
    "    merge_layer1 = layers.Lambda(cosine_similarity)([tower_1[\"enc\"], tower_2[\"enc\"]])\n",
    "    merge_layer2 = layers.Lambda(cosine_similarity)([tower_1[\"enc\"], tower_3[\"enc\"]])\n",
    "    merge_layer3 = layers.Lambda(cosine_similarity)([tower_2[\"enc\"], tower_3[\"enc\"]])\n",
    "#     merge_layer = layers.Lambda(euclidean_distance)([tower_1[\"enc\"], tower_2[\"enc\"]])\n",
    "    norm_layer1 = layers.BatchNormalization()(merge_layer1)\n",
    "    norm_layer2 = layers.BatchNormalization()(merge_layer2)\n",
    "    norm_layer3 = layers.BatchNormalization()(merge_layer3)\n",
    "    \n",
    "    siamese_logits = {}\n",
    "    siamese_logits[\"ctr1\"] = layers.Dense(1, activation=\"sigmoid\", name=\"ctr1\")(norm_layer1)\n",
    "    siamese_logits[\"ctr2\"] = layers.Dense(1, activation=\"sigmoid\", name=\"ctr2\")(norm_layer2)\n",
    "    siamese_logits[\"ctr3\"] = layers.Dense(1, activation=\"sigmoid\", name=\"ctr3\")(norm_layer3)\n",
    "    siamese_logits[\"enc\"] = layers.Layer(name=\"enc\")(tower_1[\"enc\"])\n",
    "    siamese_logits[\"dec1\"] = layers.Layer(name=\"dec1\")(tower_1[\"dec\"])\n",
    "    siamese_logits[\"dec2\"] = layers.Layer(name=\"dec2\")(tower_2[\"dec\"])\n",
    "    siamese_logits[\"dec3\"] = layers.Layer(name=\"dec3\")(tower_3[\"dec\"])\n",
    "    \n",
    "    siamese = CustomModel(inputs=[inp_1, inp_2, inp_3], outputs=siamese_logits)\n",
    "    \n",
    "    siamese.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss={\n",
    "            'ctr1': loss(),\n",
    "            'ctr2': loss(),\n",
    "            'ctr3': loss(),\n",
    "            'dec1': losses.MeanSquaredError(),\n",
    "            'dec2': losses.MeanSquaredError(),\n",
    "            'dec3': losses.MeanSquaredError(),\n",
    "            'enc': losses.BinaryCrossentropy()\n",
    "        },\n",
    "        metrics={\n",
    "            'enc': metrics.Accuracy(),\n",
    "        },\n",
    "        experimental_run_tf_function=False\n",
    "    )\n",
    "    \n",
    "    return siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp_1 (InputLayer)              [(None, 96, 160, 160 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inp_2 (InputLayer)              [(None, 96, 160, 160 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inp_3 (InputLayer)              [(None, 96, 160, 160 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "custom_model (CustomModel)      {'enc': (None, 10),  269087      inp_1[0][0]                      \n",
      "                                                                 inp_2[0][0]                      \n",
      "                                                                 inp_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           custom_model[1][1]               \n",
      "                                                                 custom_model[2][1]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           custom_model[1][1]               \n",
      "                                                                 custom_model[3][1]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           custom_model[2][1]               \n",
      "                                                                 custom_model[3][1]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 1)            4           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 1)            4           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 1)            4           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "ctr1 (Dense)                    (None, 1)            2           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "ctr2 (Dense)                    (None, 1)            2           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "ctr3 (Dense)                    (None, 1)            2           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Layer)                    (None, 96, 160, 160, 0           custom_model[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dec2 (Layer)                    (None, 96, 160, 160, 0           custom_model[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dec3 (Layer)                    (None, 96, 160, 160, 0           custom_model[3][0]               \n",
      "__________________________________________________________________________________________________\n",
      "enc (Layer)                     (None, 10)           0           custom_model[1][1]               \n",
      "==================================================================================================\n",
      "Total params: 269,105\n",
      "Trainable params: 268,779\n",
      "Non-trainable params: 326\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model(get_inputs())\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    \"\"\"\n",
    "    Method to define a Python generator for training data\n",
    "    \n",
    "    \"\"\"\n",
    "    # --- Define lambda function for random values [-0.5, +0.5]\n",
    "    lo = -0.5\n",
    "    hi = +0.5\n",
    "    rand = lambda shape : np.random.rand(*shape) * (hi - lo) + lo\n",
    "\n",
    "    # --- Define lambda function for linear transform\n",
    "    m = 2\n",
    "    b = -1\n",
    "    f = lambda x : m * x + b + rand((1, 96, 160, 160, 1))\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        xs = {}\n",
    "        xs['inp_1'] = np.expand_dims(rand((4, 96, 160, 160)), -1)\n",
    "        xs['inp_2'] = np.expand_dims(rand((4, 96, 160, 160)), -1)\n",
    "        xs['inp_3'] = np.expand_dims(rand((4, 96, 160, 160)), -1)\n",
    "        \n",
    "#         xs['dat'] = ((batch_size, 3, 96, 160, 160, 1))\n",
    "\n",
    "        ys = {}\n",
    "        ys['ctr1'] = rand((4, 1))\n",
    "        ys['ctr2'] = rand((4, 1))\n",
    "        ys['ctr3'] = rand((4, 1))\n",
    "        ys['enc'] = rand((4, 10))\n",
    "        ys['dec1'] = rand((4, 96, 160, 160, 1))\n",
    "        ys['dec2'] = rand((4, 96, 160, 160, 1))\n",
    "        ys['dec3'] = rand((4, 96, 160, 160, 1))\n",
    "                \n",
    "        yield xs, ys\n",
    "        \n",
    "gen_train = Generator()\n",
    "gen_valid = Generator()\n",
    "gen_test = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 152s 30s/step - loss: 5.9265 - ctr1_loss: 0.2822 - ctr2_loss: 0.2496 - ctr3_loss: 0.2488 - dec1_loss: 1.5317 - dec2_loss: 1.5320 - dec3_loss: 1.5325 - enc_loss: 0.5495 - enc_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f712013dfd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_logger = callbacks.CSVLogger(filename=\"training_log.csv\")\n",
    "\n",
    "# --- Train\n",
    "model.fit(\n",
    "    x=gen_train,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=5,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=10,\n",
    "    validation_freq=5#,\n",
    "#     callbacks=[csv_logger]\n",
    "#     callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn={\n",
    "    'ctr1': loss(),\n",
    "    'ctr2': loss(),\n",
    "    'ctr3': loss(),\n",
    "    'dec1': losses.MeanSquaredError(),\n",
    "    'dec2': losses.MeanSquaredError(),\n",
    "    'dec3': losses.MeanSquaredError(),\n",
    "    'enc': losses.BinaryCrossentropy()\n",
    "}\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = metrics.Accuracy()\n",
    "val_acc_metric = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 4.6364\n",
      "Seen so far: 4 samples\n",
      "Training loss (for one batch) at step 1: 4.6669\n",
      "Seen so far: 8 samples\n",
      "Training loss (for one batch) at step 2: 4.2962\n",
      "Seen so far: 12 samples\n",
      "Training loss (for one batch) at step 3: 3.9099\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 4: 3.3724\n",
      "Seen so far: 20 samples\n",
      "Training loss (for one batch) at step 5: 3.2586\n",
      "Seen so far: 24 samples\n",
      "Training loss (for one batch) at step 6: 3.5254\n",
      "Seen so far: 28 samples\n",
      "Training loss (for one batch) at step 7: 3.3762\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 8: 2.7795\n",
      "Seen so far: 36 samples\n",
      "Training loss (for one batch) at step 9: 3.6156\n",
      "Seen so far: 40 samples\n",
      "Training acc over epoch: 0.0000\n",
      "Time taken: 956.13s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.7321\n",
      "Seen so far: 4 samples\n",
      "Training loss (for one batch) at step 1: 2.1212\n",
      "Seen so far: 8 samples\n",
      "Training loss (for one batch) at step 2: 2.8035\n",
      "Seen so far: 12 samples\n",
      "Training loss (for one batch) at step 3: 1.7198\n",
      "Seen so far: 16 samples\n",
      "Training loss (for one batch) at step 4: 1.3816\n",
      "Seen so far: 20 samples\n",
      "Training loss (for one batch) at step 5: 1.2758\n",
      "Seen so far: 24 samples\n",
      "Training loss (for one batch) at step 6: 1.0432\n",
      "Seen so far: 28 samples\n",
      "Training loss (for one batch) at step 7: 1.2803\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 8: 1.6994\n",
      "Seen so far: 36 samples\n",
      "Training loss (for one batch) at step 9: 1.5745\n",
      "Seen so far: 40 samples\n",
      "Training acc over epoch: 0.0000\n",
      "Time taken: 959.04s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(gen_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            enc_loss = loss_fn['enc'](y_batch_train['enc'], logits['enc'])\n",
    "            dec1_loss = loss_fn['dec1'](y_batch_train['dec1'], logits['dec1'])\n",
    "            dec2_loss = loss_fn['dec2'](y_batch_train['dec2'], logits['dec2'])\n",
    "            dec3_loss = loss_fn['dec3'](y_batch_train['dec3'], logits['dec3'])\n",
    "            ctr1_loss = loss_fn['ctr1'](y_batch_train['ctr1'], logits['ctr1'])\n",
    "            ctr2_loss = loss_fn['ctr2'](y_batch_train['ctr2'], logits['ctr2'])\n",
    "            ctr3_loss = loss_fn['ctr3'](y_batch_train['ctr3'], logits['ctr3'])\n",
    "            \n",
    "            loss_value = enc_loss + dec1_loss + dec2_loss + dec3_loss + ctr1_loss + ctr2_loss + ctr3_loss\n",
    "#             loss_value = 0.5 * enc_loss + 0.2 * dec_loss + 0.3 * ctr_loss\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train['enc'], logits['enc'])\n",
    "\n",
    "        # Log every 10 batches.\n",
    "        if step == 10:\n",
    "            break\n",
    "        print(\n",
    "            \"Training loss (for one batch) at step %d: %.4f\"\n",
    "            % (step, float(loss_value))\n",
    "        )\n",
    "        print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "#     for x_batch_val, y_batch_val in val_dataset:\n",
    "#         val_logits = model(x_batch_val, training=False)\n",
    "#         # Update val metrics\n",
    "#         val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "#     val_acc = val_acc_metric.result()\n",
    "#     val_acc_metric.reset_states()\n",
    "#     print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
