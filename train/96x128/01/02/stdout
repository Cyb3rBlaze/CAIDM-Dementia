2021-07-29 12:03:56.643671: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-07-29 12:03:56.643746: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-07-29 12:03:56.643753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2021-07-29 12:03:57.836289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-07-29 12:03:57.871582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:43:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-07-29 12:03:57.871864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-07-29 12:03:57.873798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2021-07-29 12:03:57.875755: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2021-07-29 12:03:57.876033: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2021-07-29 12:03:57.877974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2021-07-29 12:03:57.879034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2021-07-29 12:03:57.883089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-07-29 12:03:57.886400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-07-29 12:03:57.886876: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-07-29 12:03:57.895455: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000125000 Hz
2021-07-29 12:03:57.897891: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bb3504c9f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-07-29 12:03:57.897930: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-07-29 12:03:57.997461: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bb35048830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-07-29 12:03:57.997488: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2021-07-29 12:03:57.998546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:43:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-07-29 12:03:57.998592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-07-29 12:03:57.998604: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2021-07-29 12:03:57.998614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2021-07-29 12:03:57.998624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2021-07-29 12:03:57.998637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2021-07-29 12:03:57.998646: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2021-07-29 12:03:57.998656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-07-29 12:03:58.000456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-07-29 12:03:58.000482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2021-07-29 12:03:58.001877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-07-29 12:03:58.001886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2021-07-29 12:03:58.001893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2021-07-29 12:03:58.003776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10320 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:43:00.0, compute capability: 7.5)
2021-07-29 12:04:00.328507: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-07-29 12:04:01.038387: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2021-07-29 12:04:01.088052: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
[ 2021-07-29 12:03:58 ] CUDA_VISIBLE_DEVICES automatically set to: 1           

Start of epoch 0
Training loss (for one batch) at step 0: 12.0429
Seen so far: 3 samples
Training loss (for one batch) at step 1: 12.1978
Seen so far: 6 samples
Training loss (for one batch) at step 2: 14.8967
Seen so far: 9 samples
Training loss (for one batch) at step 3: 12.4716
Seen so far: 12 samples
Training loss (for one batch) at step 4: 11.2414
Seen so far: 15 samples
Training loss (for one batch) at step 5: 10.6480
Seen so far: 18 samples
Training loss (for one batch) at step 6: 10.3420
Seen so far: 21 samples
Training loss (for one batch) at step 7: 9.8562
Seen so far: 24 samples
Training loss (for one batch) at step 8: 11.1738
Seen so far: 27 samples
Training loss (for one batch) at step 9: 11.1168
Seen so far: 30 samples
Training loss (for one batch) at step 10: 11.0088
Seen so far: 33 samples
Training loss (for one batch) at step 11: 11.3730
Seen so far: 36 samples
Training loss (for one batch) at step 12: 11.6289
Seen so far: 39 samples
Training loss (for one batch) at step 13: 10.5399
Seen so far: 42 samples
Training loss (for one batch) at step 14: 10.9075
Seen so far: 45 samples
Training loss (for one batch) at step 15: 10.2148
Seen so far: 48 samples
Training loss (for one batch) at step 16: 10.9278
Seen so far: 51 samples
Training loss (for one batch) at step 17: 10.2747
Seen so far: 54 samples
Training loss (for one batch) at step 18: 9.8481
Seen so far: 57 samples
Training loss (for one batch) at step 19: 9.2805
Seen so far: 60 samples
Training loss (for one batch) at step 20: 10.5747
Seen so far: 63 samples
Training loss (for one batch) at step 21: 10.2976
Seen so far: 66 samples
Training loss (for one batch) at step 22: 9.6941
Seen so far: 69 samples
Training loss (for one batch) at step 23: 8.9754
Seen so far: 72 samples
Training loss (for one batch) at step 24: 9.7410
Seen so far: 75 samples
Training loss (for one batch) at step 25: 9.4983
Seen so far: 78 samples
Training loss (for one batch) at step 26: 9.8958
Seen so far: 81 samples
Training loss (for one batch) at step 27: 9.1621
Seen so far: 84 samples
Training loss (for one batch) at step 28: 8.9228
Seen so far: 87 samples
Training loss (for one batch) at step 29: 8.5642
Seen so far: 90 samples
Training loss (for one batch) at step 30: 9.7497
Seen so far: 93 samples
Training loss (for one batch) at step 31: 8.7482
Seen so far: 96 samples
Training loss (for one batch) at step 32: 8.9796
Seen so far: 99 samples
Training loss (for one batch) at step 33: 8.6537
Seen so far: 102 samples
Training loss (for one batch) at step 34: 8.7094
Seen so far: 105 samples
Training loss (for one batch) at step 35: 8.8447
Seen so far: 108 samples
Training loss (for one batch) at step 36: 9.2673
Seen so far: 111 samples
Training loss (for one batch) at step 37: 8.4988
Seen so far: 114 samples
Training loss (for one batch) at step 38: 7.7683
Seen so far: 117 samples
Training loss (for one batch) at step 39: 8.4771
Seen so far: 120 samples
Training loss (for one batch) at step 40: 8.7236
Seen so far: 123 samples
Training loss (for one batch) at step 41: 8.5321
Seen so far: 126 samples
Training loss (for one batch) at step 42: 7.8564
Seen so far: 129 samples
Training loss (for one batch) at step 43: 8.2816
Seen so far: 132 samples
Training loss (for one batch) at step 44: 7.6600
Seen so far: 135 samples
Training loss (for one batch) at step 45: 7.9846
Seen so far: 138 samples
Training loss (for one batch) at step 46: 7.9738
Seen so far: 141 samples
Training loss (for one batch) at step 47: 8.0049
Seen so far: 144 samples
Training loss (for one batch) at step 48: 8.1744
Seen so far: 147 samples
Training loss (for one batch) at step 49: 6.8756
Seen so far: 150 samples
Training loss (for one batch) at step 50: 7.3141
Seen so far: 153 samples
Training loss (for one batch) at step 51: 6.9114
Seen so far: 156 samples
Training loss (for one batch) at step 52: 7.0271
Seen so far: 159 samples
Training loss (for one batch) at step 53: 7.2304
Seen so far: 162 samples
Training loss (for one batch) at step 54: 7.1801
Seen so far: 165 samples
Training loss (for one batch) at step 55: 7.4032
Seen so far: 168 samples
Training loss (for one batch) at step 56: 6.3926
Seen so far: 171 samples
Training loss (for one batch) at step 57: 6.3683
Seen so far: 174 samples
Training loss (for one batch) at step 58: 7.0389
Seen so far: 177 samples
Training loss (for one batch) at step 59: 6.7987
Seen so far: 180 samples
Training loss (for one batch) at step 60: 7.4564
Seen so far: 183 samples
Training loss (for one batch) at step 61: 7.0921
Seen so far: 186 samples
Training loss (for one batch) at step 62: 5.5779
Seen so far: 189 samples
Training loss (for one batch) at step 63: 6.7477
Seen so far: 192 samples
Training loss (for one batch) at step 64: 6.8863
Seen so far: 195 samples
Training loss (for one batch) at step 65: 8.0757
Seen so far: 198 samples
Training loss (for one batch) at step 66: 6.1659
Seen so far: 201 samples
Training loss (for one batch) at step 67: 6.3044
Seen so far: 204 samples
Training loss (for one batch) at step 68: 6.3804
Seen so far: 207 samples
Training loss (for one batch) at step 69: 5.8317
Seen so far: 210 samples
Training loss (for one batch) at step 70: 5.4212
Seen so far: 213 samples
Training loss (for one batch) at step 71: 5.6385
Seen so far: 216 samples
Training loss (for one batch) at step 72: 5.6190
Seen so far: 219 samples
Training loss (for one batch) at step 73: 6.4763
Seen so far: 222 samples
Training loss (for one batch) at step 74: 5.8340
Seen so far: 225 samples
Training loss (for one batch) at step 75: 5.6217
Seen so far: 228 samples
Training loss (for one batch) at step 76: 5.5179
Seen so far: 231 samples
Training loss (for one batch) at step 77: 6.2400
Seen so far: 234 samples
Training loss (for one batch) at step 78: 5.0967
Seen so far: 237 samples
Training loss (for one batch) at step 79: 5.2915
Seen so far: 240 samples
Training loss (for one batch) at step 80: 5.1818
Seen so far: 243 samples
Training loss (for one batch) at step 81: 5.1394
Seen so far: 246 samples
Training loss (for one batch) at step 82: 5.3879
Seen so far: 249 samples
Training loss (for one batch) at step 83: 5.5305
Seen so far: 252 samples
Training loss (for one batch) at step 84: 5.1356
Seen so far: 255 samples
Training loss (for one batch) at step 85: 5.0684
Seen so far: 258 samples
Training loss (for one batch) at step 86: 4.7698
Seen so far: 261 samples
Training loss (for one batch) at step 87: 4.7287
Seen so far: 264 samples
Training loss (for one batch) at step 88: 4.8914
Seen so far: 267 samples
Training loss (for one batch) at step 89: 4.6325
Seen so far: 270 samples
Training loss (for one batch) at step 90: 5.0888
Seen so far: 273 samples
Training loss (for one batch) at step 91: 4.5192
Seen so far: 276 samples
Training loss (for one batch) at step 92: 4.9412
Seen so far: 279 samples
Training loss (for one batch) at step 93: 4.4995
Seen so far: 282 samples
Training loss (for one batch) at step 94: 4.9373
Seen so far: 285 samples
Training loss (for one batch) at step 95: 4.4804
Seen so far: 288 samples
Training loss (for one batch) at step 96: 4.5633
Seen so far: 291 samples
Training loss (for one batch) at step 97: 4.8144
Seen so far: 294 samples
Training loss (for one batch) at step 98: 4.7136
Seen so far: 297 samples
Training loss (for one batch) at step 99: 4.2775
Seen so far: 300 samples
Training loss (for one batch) at step 100: 4.7535
Seen so far: 303 samples
Training acc over epoch:
 0.6138613820075989 
 0.6336633563041687 
 0.594059407711029
Validation acc:
 0.6831682920455933 
 0.6435643434524536 
 0.48514851927757263
Time taken: 102.55s

Start of epoch 1
Training loss (for one batch) at step 0: 4.3265
Seen so far: 3 samples
Training loss (for one batch) at step 1: 5.4740
Seen so far: 6 samples
Training loss (for one batch) at step 2: 4.4980
Seen so far: 9 samples
Training loss (for one batch) at step 3: 4.8354
Seen so far: 12 samples
Training loss (for one batch) at step 4: 5.0071
Seen so far: 15 samples
Training loss (for one batch) at step 5: 4.5057
Seen so far: 18 samples
Training loss (for one batch) at step 6: 4.3456
Seen so far: 21 samples
Training loss (for one batch) at step 7: 4.0067
Seen so far: 24 samples
Training loss (for one batch) at step 8: 4.6152
Seen so far: 27 samples
Training loss (for one batch) at step 9: 4.3262
Seen so far: 30 samples
Training loss (for one batch) at step 10: 4.4953
Seen so far: 33 samples
Training loss (for one batch) at step 11: 4.0210
Seen so far: 36 samples
Training loss (for one batch) at step 12: 4.2232
Seen so far: 39 samples
Training loss (for one batch) at step 13: 3.4649
Seen so far: 42 samples
Training loss (for one batch) at step 14: 3.7878
Seen so far: 45 samples
Training loss (for one batch) at step 15: 3.7796
Seen so far: 48 samples
Training loss (for one batch) at step 16: 4.2107
Seen so far: 51 samples
Training loss (for one batch) at step 17: 4.2543
Seen so far: 54 samples
Training loss (for one batch) at step 18: 4.1484
Seen so far: 57 samples
Training loss (for one batch) at step 19: 4.1845
Seen so far: 60 samples
Training loss (for one batch) at step 20: 3.8548
Seen so far: 63 samples
Training loss (for one batch) at step 21: 4.1050
Seen so far: 66 samples
Training loss (for one batch) at step 22: 3.6713
Seen so far: 69 samples
Training loss (for one batch) at step 23: 3.6304
Seen so far: 72 samples
Training loss (for one batch) at step 24: 3.6073
Seen so far: 75 samples
Training loss (for one batch) at step 25: 3.5002
Seen so far: 78 samples
Training loss (for one batch) at step 26: 4.0342
Seen so far: 81 samples
Training loss (for one batch) at step 27: 3.5150
Seen so far: 84 samples
Training loss (for one batch) at step 28: 3.8416
Seen so far: 87 samples
Training loss (for one batch) at step 29: 4.0320
Seen so far: 90 samples
Training loss (for one batch) at step 30: 3.2064
Seen so far: 93 samples
Training loss (for one batch) at step 31: 2.9552
Seen so far: 96 samples
Training loss (for one batch) at step 32: 3.8641
Seen so far: 99 samples
Training loss (for one batch) at step 33: 3.8223
Seen so far: 102 samples
Training loss (for one batch) at step 34: 3.9267
Seen so far: 105 samples
Training loss (for one batch) at step 35: 4.5067
Seen so far: 108 samples
Training loss (for one batch) at step 36: 4.5626
Seen so far: 111 samples
Training loss (for one batch) at step 37: 3.7046
Seen so far: 114 samples
Training loss (for one batch) at step 38: 3.7742
Seen so far: 117 samples
Training loss (for one batch) at step 39: 3.1588
Seen so far: 120 samples
Training loss (for one batch) at step 40: 3.5641
Seen so far: 123 samples
Training loss (for one batch) at step 41: 2.8461
Seen so far: 126 samples
Training loss (for one batch) at step 42: 3.7453
Seen so far: 129 samples
Training loss (for one batch) at step 43: 4.2156
Seen so far: 132 samples
Training loss (for one batch) at step 44: 3.0813
Seen so far: 135 samples
Training loss (for one batch) at step 45: 4.8790
Seen so far: 138 samples
Training loss (for one batch) at step 46: 3.4421
Seen so far: 141 samples
Training loss (for one batch) at step 47: 3.0416
Seen so far: 144 samples
Training loss (for one batch) at step 48: 4.5841
Seen so far: 147 samples
Training loss (for one batch) at step 49: 3.5684
Seen so far: 150 samples
Training loss (for one batch) at step 50: 3.8513
Seen so far: 153 samples
Training loss (for one batch) at step 51: 3.1700
Seen so far: 156 samples
Training loss (for one batch) at step 52: 3.5084
Seen so far: 159 samples
Training loss (for one batch) at step 53: 3.5711
Seen so far: 162 samples
Training loss (for one batch) at step 54: 3.6798
Seen so far: 165 samples
Training loss (for one batch) at step 55: 3.0700
Seen so far: 168 samples
Training loss (for one batch) at step 56: 3.4940
Seen so far: 171 samples
Training loss (for one batch) at step 57: 3.6414
Seen so far: 174 samples
Training loss (for one batch) at step 58: 3.8272
Seen so far: 177 samples
Training loss (for one batch) at step 59: 3.1912
Seen so far: 180 samples
Training loss (for one batch) at step 60: 3.5937
Seen so far: 183 samples
Training loss (for one batch) at step 61: 4.2857
Seen so far: 186 samples
Training loss (for one batch) at step 62: 3.2809
Seen so far: 189 samples
Training loss (for one batch) at step 63: 3.3982
Seen so far: 192 samples
Training loss (for one batch) at step 64: 3.6177
Seen so far: 195 samples
Training loss (for one batch) at step 65: 3.7194
Seen so far: 198 samples
Training loss (for one batch) at step 66: 3.5270
Seen so far: 201 samples
Training loss (for one batch) at step 67: 3.3826
Seen so far: 204 samples
Training loss (for one batch) at step 68: 3.5013
Seen so far: 207 samples
Training loss (for one batch) at step 69: 2.7073
Seen so far: 210 samples
Training loss (for one batch) at step 70: 3.5428
Seen so far: 213 samples
Training loss (for one batch) at step 71: 3.3847
Seen so far: 216 samples
Training loss (for one batch) at step 72: 3.1358
Seen so far: 219 samples
Training loss (for one batch) at step 73: 2.9239
Seen so far: 222 samples
Training loss (for one batch) at step 74: 3.3778
Seen so far: 225 samples
Training loss (for one batch) at step 75: 3.2531
Seen so far: 228 samples
Training loss (for one batch) at step 76: 2.9038
Seen so far: 231 samples
Training loss (for one batch) at step 77: 3.4222
Seen so far: 234 samples
Training loss (for one batch) at step 78: 3.4896
Seen so far: 237 samples
Training loss (for one batch) at step 79: 2.5472
Seen so far: 240 samples
Training loss (for one batch) at step 80: 3.2778
Seen so far: 243 samples
Training loss (for one batch) at step 81: 2.8029
Seen so far: 246 samples
Training loss (for one batch) at step 82: 3.2747
Seen so far: 249 samples
Training loss (for one batch) at step 83: 2.9559
Seen so far: 252 samples
Training loss (for one batch) at step 84: 3.7945
Seen so far: 255 samples
Training loss (for one batch) at step 85: 3.3561
Seen so far: 258 samples
Training loss (for one batch) at step 86: 3.3948
Seen so far: 261 samples
Training loss (for one batch) at step 87: 3.5271
Seen so far: 264 samples
Training loss (for one batch) at step 88: 3.3357
Seen so far: 267 samples
Training loss (for one batch) at step 89: 3.4202
Seen so far: 270 samples
Training loss (for one batch) at step 90: 3.3184
Seen so far: 273 samples
Training loss (for one batch) at step 91: 5.2147
Seen so far: 276 samples
Training loss (for one batch) at step 92: 3.4092
Seen so far: 279 samples
Training loss (for one batch) at step 93: 3.3058
Seen so far: 282 samples
Training loss (for one batch) at step 94: 2.8014
Seen so far: 285 samples
Training loss (for one batch) at step 95: 3.4329
Seen so far: 288 samples
Training loss (for one batch) at step 96: 4.9389
Seen so far: 291 samples
Training loss (for one batch) at step 97: 2.8825
Seen so far: 294 samples
Training loss (for one batch) at step 98: 2.8435
Seen so far: 297 samples
Training loss (for one batch) at step 99: 3.5367
Seen so far: 300 samples
Training loss (for one batch) at step 100: 2.4497
Seen so far: 303 samples
Training acc over epoch:
 0.8316831588745117 
 0.6534653306007385 
 0.39603960514068604
Validation acc:
 0.8415841460227966 
 0.6732673048973083 
 0.4356435537338257
Time taken: 96.16s
