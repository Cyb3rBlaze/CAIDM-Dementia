{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Dementia Notebook\n",
    "\n",
    "Rough draft of how multiple data sources may be loaded with jarvis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import losses, optimizers, metrics\n",
    "from tensorflow.keras import Input, Model, layers, callbacks, regularizers\n",
    "from jarvis.train import custom, datasets, params\n",
    "from jarvis.train.client import Client\n",
    "from jarvis.utils.general import gpus, overload, tools as jtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Overload jarvis client\n",
    "@overload(Client)\n",
    "def preprocess(self, arrays, **kwargs):\n",
    "    \"\"\"\n",
    "    Method to create a custom msk array for class weights and/or masks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Binarize data into two classes\n",
    "    arrays['ys']['lbl-anc'] = arrays['ys']['lbl-anc'] >= 1\n",
    "    arrays['ys']['lbl-neg'] = arrays['ys']['lbl-neg'] >= 1\n",
    "    arrays['ys']['lbl-pos'] = arrays['ys']['lbl-pos'] >= 1\n",
    "    \n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(inputs):\n",
    "\t(x1, x2) = inputs\n",
    "\tsumSquared = K.sum(K.square(x1 - x2), axis=1, keepdims=True)\n",
    "\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
    "\n",
    "def subtract(inputs):\n",
    "\t(x1, x2) = inputs\n",
    "\treturn x1-x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_client(paths, p):\n",
    "\n",
    "    client = Client(CLIENT_TRAINING if os.path.exists(CLIENT_TRAINING) else CLIENT_TEMPLATE, configs={\n",
    "        'batch': {\n",
    "            'size': p['batch_size'],\n",
    "            'fold': p['fold']}})\n",
    "\n",
    "    return client\n",
    "\n",
    "def prepare_model(inputs):\n",
    "\n",
    "    # --- Define kwargs dictionary\n",
    "    kwargs = {\n",
    "        'kernel_size': (3, 3, 3),\n",
    "        'padding': 'same',\n",
    "        'kernel_initializer': 'he_uniform'}\n",
    "\n",
    "    # --- Add kernel regularizer if set\n",
    "    if p['kernel_regularizer'] != 0:\n",
    "        kwargs['kernel_regularizer'] = regularizers.l2(p['kernel_regularizer'])\n",
    "\n",
    "    # --- Define lambda functions\n",
    "    conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "    norm = lambda x : layers.BatchNormalization()(x)\n",
    "    elu = lambda x : layers.ELU()(x)\n",
    "    tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
    "    drop = lambda x : layers.Dropout(rate=p['dropout'])(x)\n",
    "\n",
    "    # --- Define stride-1, stride-2 blocks\n",
    "    conv1 = lambda filters, x : norm(elu(conv(x, filters, strides=1)))\n",
    "    conv2 = lambda filters, x : norm(elu(conv(x, filters, strides=(2, 2, 2))))\n",
    "    tran2 = lambda filters, x : norm(elu(tran(x, filters, strides=(2, 2, 2))))\n",
    "\n",
    "    # --- Extract alpha value\n",
    "    a = p['alpha']\n",
    "    \n",
    "    # --- Define dummy anchor model\n",
    "    a1 = conv1(int(a*8), inputs['anc'])\n",
    "    a2 = conv1(int(a*16), conv2(int(a*16), a1))\n",
    "    a3 = tran2(int(a*8), a2)\n",
    "    a4 = conv1(int(a*8), conv1(int(a*8), a3))\n",
    "    \n",
    "    encoder_unknown = a2\n",
    "    decoder_unknown = a4\n",
    "    \n",
    "    # --- Define dummy control model\n",
    "    n1 = conv1(int(a*8), inputs['neg'])\n",
    "    n2 = conv1(int(a*16), conv2(int(a*16), n1))\n",
    "    n3 = tran2(int(a*8), n2)\n",
    "    n4 = conv1(int(a*8), conv1(int(a*8), n3))\n",
    "    \n",
    "    encoder_cn = n2\n",
    "    decoder_cn = n4\n",
    "    \n",
    "    # --- Define dummy AD model\n",
    "    p1 = conv1(int(a*8), inputs['pos'])\n",
    "    p2 = conv1(int(a*16), conv2(int(a*16), p1))\n",
    "    p3 = tran2(int(a*8), p2)\n",
    "    p4 = conv1(int(a*8), conv1(int(a*8), p3))\n",
    "    \n",
    "    encoder_ad = p2\n",
    "    decoder_ad = p4\n",
    "    \n",
    "    # --- Create logits\n",
    "    logits = {}\n",
    "    logits['lbl-anc'] = layers.Conv3D(filters=2, name='lbl-anc', kernel_size=(1, 1, 1), padding='same')(a4)\n",
    "    logits['lbl-neg'] = layers.Conv3D(filters=2, name='lbl-neg', kernel_size=(1, 1, 1), padding='same')(n4)\n",
    "    logits['lbl-pos'] = layers.Conv3D(filters=2, name='lbl-pos', kernel_size=(1, 1, 1), padding='same')(p4)\n",
    "    \n",
    "    # --- Compute Euclidean distance\n",
    "    ad_unknown = layers.Lambda(euclidean_distance)([encoder_ad, encoder_unknown])\n",
    "    cn_unknown = layers.Lambda(euclidean_distance)([encoder_cn, encoder_unknown])\n",
    "    \n",
    "    final_difference = layers.Lambda(subtract)([ad_unknown, cn_unknown])\n",
    "    flatten_difference = layers.Flatten()(final_difference)\n",
    "    final_activation = layers.Dense(1, name=\"final_activation\", activation=\"sigmoid\")(flatten_difference)\n",
    "\n",
    "    # --- Create model\n",
    "    model = Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "    # --- Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=p['LR']),\n",
    "        loss={\n",
    "            'lbl-anc': losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            'lbl-neg': losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            'lbl-pos': losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        },\n",
    "        metrics={\n",
    "            'lbl-anc': custom.dsc(cls=1),\n",
    "            'lbl-neg': custom.dsc(cls=1),\n",
    "            'lbl-pos': custom.dsc(cls=1)\n",
    "        },\n",
    "        experimental_run_tf_function=False\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_existing(model, p):\n",
    "\n",
    "    # --- Create output_dir\n",
    "    os.makedirs(p['output_dir'], exist_ok=True)\n",
    "\n",
    "    # --- Load existing model if present\n",
    "    if os.path.exists(MODEL_NAME):\n",
    "        print('Loading existing model weights: {}'.format(MODEL_NAME))\n",
    "        model.load_weights(MODEL_NAME)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Autoselect GPU\n",
    "#gpus.autoselect()\n",
    "\n",
    "# --- Look up path\n",
    "paths = jtools.get_paths('ct/kidney')\n",
    "\n",
    "# --- Prepare hyperparams\n",
    "p = params.load('~/projects/dementia_test/hyper.csv', row=0)\n",
    "\n",
    "# --- Set constants\n",
    "CLIENT_TEMPLATE = '/home/mmorelan/projects/dementia_test/yml/client-dementia-3d.yml' # TODO: Replace this with your user path / global path to YML file\n",
    "CLIENT_TRAINING = '{}/client.yml'.format(p['output_dir'])\n",
    "MODEL_NAME = '{}/model.hdf5'.format(p['output_dir'])\n",
    "\n",
    "# --- Prepare client\n",
    "client = prepare_client(paths, p)\n",
    "# client.load_data_in_memory()\n",
    "gen_train, gen_valid = client.create_generators()\n",
    "\n",
    "# --- Prepare model\n",
    "model = prepare_model(client.get_inputs(Input))\n",
    "model = load_existing(model, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assume a 400:100 ratio of train:valid\n",
    "steps_per_epoch = 50\n",
    "validation_freq = 1\n",
    "\n",
    "# --- Determine total loop iterations needed\n",
    "epochs = int(p['iterations'] / steps_per_epoch)\n",
    "\n",
    "# --- Prepare Tensorboard\n",
    "log_dir = '{}/jmodels/logdirs/{}'.format(\n",
    "    os.path.dirname(p['output_dir']),\n",
    "    os.path.basename(p['output_dir']))\n",
    "\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=log_dir, profile_batch=0)\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# --- Train\n",
    "model.fit(\n",
    "    x=gen_train, \n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=steps_per_epoch,\n",
    "    validation_freq=validation_freq,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# --- Save model\n",
    "model.save(MODEL_NAME)\n",
    "\n",
    "# --- Save client\n",
    "client.to_yml(CLIENT_TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
